{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12566997,"sourceType":"datasetVersion","datasetId":6540683}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# EY Challenge","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:19:09.227555Z","iopub.execute_input":"2025-08-01T13:19:09.231228Z","iopub.status.idle":"2025-08-01T13:19:09.832305Z","shell.execute_reply.started":"2025-08-01T13:19:09.231100Z","shell.execute_reply":"2025-08-01T13:19:09.831273Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# These are commands for installing GDAL and Optuna in Colab or Linux environments.\n# They are left here for reference and are not executed automatically.\n'''\n!apt-get update\n!apt-get install -y gdal-bin\n!pip install optuna\n'''\n\n# Required Python Packages (Install if not already installed)\n!pip install contextily           # For adding basemaps to spatial plots\n!pip install inequality           # For calculating Gini coefficients\n\n# Data Handling and Utilities\nimport pandas as pd               # DataFrames and analysis\nimport numpy as np                # Numerical computing\nimport geopandas as gpd           # Spatial data handling (GeoDataFrames)\nimport random                     # Random number utilities\nimport datetime                   # Date/time utilities\nimport warnings                   # Suppress warning messages\nfrom collections import OrderedDict  # Ordered dictionaries\nfrom numpy.linalg import LinAlgError  # Linear algebra error handling\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt   # Plotting\nimport seaborn as sns             # Statistical plotting\nimport missingno as msno          # Missing data visualization\nimport matplotlib.dates as mdates # Date formatting for plots\nimport matplotlib.patches as mpatches  # Custom plot shapes and legends\nimport contextily as ctx          # Add web basemaps to geospatial plots\nimport folium                     # Interactive maps\nimport branca.colormap as cm      # Custom color scales for folium maps\n\n# Geospatial and Statistical Tools\nfrom shapely.geometry import Point        # Create geometric objects (points, polygons, etc.)\nfrom inequality.gini import Gini          # Compute Gini coefficients for inequality analysis\nfrom scipy import stats                   # Statistical functions\nfrom scipy.stats import iqr, skew, mstats, kurtosis  # More descriptive stats\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor  # Multicollinearity checks\nfrom statsmodels.tools.tools import add_constant  # Adds intercept term to regression inputs\n\n# Machine Learning and Modeling\nfrom sklearn.preprocessing import PowerTransformer, StandardScaler\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import (\n    train_test_split, cross_val_score, cross_val_predict,\n    KFold, TimeSeriesSplit\n)\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.linear_model import Ridge, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\nfrom sklearn.neighbors import NearestNeighbors\n\n# Gradient Boosting Models\nimport xgboost as xgb                # High-performance gradient boosting\nimport lightgbm as lgb              # LightGBM for fast tree boosting\n\n# Interactive Visualization (Jupyter Widgets)\nfrom ipywidgets import interact, IntSlider    # Create interactive widgets\nfrom IPython.display import display           # Display widget outputs in notebook\n\n# Global Settings and Display Configurations\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\n# Improve display formatting in pandas\npd.set_option('display.float_format', lambda x: '%.2f' % x)  # Avoid scientific notation\npd.set_option('display.max_rows', None)                      # Show all DataFrame rows\npd.set_option('display.max_columns', None)                   # Show all DataFrame columns\npd.set_option('display.width', None)                         # Disable line wrapping\npd.set_option('display.max_colwidth', None)                  # Show full content in cells\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:19:16.706259Z","iopub.execute_input":"2025-08-01T13:19:16.706775Z","iopub.status.idle":"2025-08-01T13:19:35.554536Z","shell.execute_reply.started":"2025-08-01T13:19:16.706745Z","shell.execute_reply":"2025-08-01T13:19:35.553281Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Dataset Loading & Initial Inspection**\n\nThis project originally followed a competition-based format, where a separate test dataset was used to evaluate model performance. However, since that context is no longer relevant, the workflow has been updated to align with standard machine learning practices. Instead of relying on a predefined test file (1,040 entries), the full dataset (11,299 entries) is now used with an **80-20 train-test split**. This change ensures proper generalization evaluation using a representative hold-out set (2,316 entries), and all modeling steps—from preprocessing to validation—are applied strictly on the training data.\n\nIn this section, I loaded the datasets and performed an initial inspection to understand their structure, data types, and overall composition. The goal was to ensure that everything was properly formatted before moving on to deeper analysis and dataset merging.\n\n\n### **Training Data Overview**\n\n* **Entries:** 11,229\n* **Columns:** 4\n* **Key Features:** Longitude, Latitude, Datetime, UHI Index\n* **Data Types:**\n\n  * 3 **float64** columns\n  * 1 **object** column (datetime, which will be converted later)\n    **No missing values detected in training data.**\n\n\n### **Bronx Weather Data Overview**\n\n* **Entries:** 169\n* **Columns:** 6\n* **Key Features:** Date/Time, Air Temperature, Humidity, Wind Speed, Wind Direction, Solar Flux\n* **Data Types:**\n\n  * 3 **float64** columns\n  * 2 **int64** columns\n  * 1 **object** column (datetime, which will be converted later)\n    **No missing values detected in Bronx weather data.**\n\n\n### **Manhattan Weather Data Overview**\n\n* **Entries:** 169\n* **Columns:** 6\n* **Key Features:** Date/Time, Air Temperature, Humidity, Wind Speed, Wind Direction, Solar Flux\n* **Data Types:**\n\n  * 3 **float64** columns\n  * 2 **int64** columns\n  * 1 **object** column (datetime, which will be converted later)\n    **No missing values detected in Manhattan weather data.**\n\n\n### **Next Steps**\n\nBefore merging these datasets, the next steps involve:\n\n1. **Further Inspection:** Checking for inconsistencies, duplicates, or formatting issues.\n2. **Datetime Conversion:** Ensuring datetime columns are in the correct format.\n3. **Handling Time Differences:** If timestamps are missing or misaligned, addressing those gaps.","metadata":{}},{"cell_type":"code","source":"def load_and_describe_data():\n    \"\"\"Load datasets and display their summary information\"\"\"\n    # Load datasets\n    data_sources = {\n        \"Dataset\": (\"/kaggle/input/ey-challenge/Training_data_uhi_index_2025-02-18.csv\", pd.read_csv),\n        \"Weather Data\": (\"/kaggle/input/ey-challenge/NY_Mesonet_Weather.xlsx\", lambda f: pd.read_excel(f, sheet_name=None))\n    }\n    \n    loaded_data = {}\n    for name, (path, loader) in data_sources.items():\n        loaded_data[name] = loader(path)\n    \n    # Process weather sheets\n    weather_sheets = loaded_data[\"Weather Data\"]\n    print(\"Available weather sheets:\", weather_sheets.keys())\n    \n    # Create dictionary of all dataframes\n    dfs = {\n        \"Dataset\": loaded_data[\"Dataset\"],\n        \"Bronx Weather\": weather_sheets[\"Bronx\"],\n        \"Manhattan Weather\": weather_sheets[\"Manhattan\"]\n    }\n    \n    # Display summary for each dataframe\n    for name, df in dfs.items():\n        print(f\"\\n{'='*50}\\n{name} Summary\\n{'='*50}\")\n        print(\"\\nFirst 5 rows:\")\n        display(df.head())\n        print(\"\\nInfo:\")\n        display(df.info())\n        print(\"\\nShape:\", df.shape)\n\n    return dfs\n\n# Load and describe all data\ndfs = load_and_describe_data()\ndf, bronx_df, manhattan_df = dfs.values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:19:54.793820Z","iopub.execute_input":"2025-08-01T13:19:54.794597Z","iopub.status.idle":"2025-08-01T13:19:55.724618Z","shell.execute_reply.started":"2025-08-01T13:19:54.794560Z","shell.execute_reply":"2025-08-01T13:19:55.723807Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Ensuring Correct DateTime Format and Consistency**  \n\nBefore verifying whether all entries correspond to *July 24, 2021*, I first ensured that the **datetime columns were correctly formatted**. This was necessary to avoid misalignment issues when merging datasets later.  \n\nAfter converting the columns to **datetime**, I performed two key validation checks:  \n\n1. **Datetime Conversion Accuracy:** Ensuring that all datetime entries were successfully transformed without introducing missing (`NaT`) values.  \n2. **Date Consistency Check:** Confirming that all records in the training and weather datasets were indeed for **July 24, 2021**. \n\n#### **Datetime Conversion Results:**  \n\n| Dataset                 | Column         | Data Type           | Missing (NaT) Values | Status |\n|-------------------------|---------------|----------------------|----------------------|--------|\n| **Train Data**          | `datetime`     | `datetime64[ns]`     | **0**                | ✅ Success |\n| **Bronx Weather Data**  | `Date / Time`  | `datetime64[ns]`     | **0**                | ✅ Success |\n| **Manhattan Weather Data** | `Date / Time`  | `datetime64[ns]`     | **0**                | ✅ Success |\n\n\n#### **Date Consistency Check:**  \n\n| Dataset                 | Unique Dates Found   | Status |\n|-------------------------|----------------------|--------|\n| **Train Data**          | `2021-07-24`         | ✅ Matches |\n| **Bronx Weather Data**  | `2021-07-24`         | ✅ Matches |\n| **Manhattan Weather Data** | `2021-07-24`         | ✅ Matches |\n\n**Final Confirmation:**  \n✔️ **All datasets are correctly formatted and contain records exclusively for July 24, 2021.**  ","metadata":{}},{"cell_type":"code","source":"def process_and_validate_datetime(dfs, target_date_str):\n    \"\"\"\n    Processes datetime columns, validates conversion, and checks if all dates match target date.\n    \n    Args:\n        dfs: Dictionary of {'name': (df, column_name, conversion_kwargs)}\n        target_date_str: Target date in 'DD-MM-YYYY' format\n    \"\"\"\n    target_date = pd.to_datetime(target_date_str, dayfirst=True).date()\n    \n    # Process datetime conversion for all datasets\n    for name, (df, col, kwargs) in dfs.items():\n        if 'str.replace' in kwargs:\n            df[col] = df[col].str.replace(kwargs.pop('str.replace'), '', regex=False)\n        df[col] = pd.to_datetime(df[col], **kwargs)\n    \n    # Validate conversion and date consistency\n    print(\"\\nDatetime Conversion Validation:\")\n    for name, (df, col, _) in dfs.items():\n        dtype = df[col].dtype\n        missing = df[col].isna().sum()\n        unique_dates = df[col].dt.date.unique()\n        all_target = all(df[col].dt.date == target_date)\n        \n        print(f\"\\nDataset: {name}\")\n        print(f\"- Column '{col}' dtype: {dtype}\")\n        print(f\"- Missing values: {missing}\")\n        print(f\"- Unique dates: {unique_dates}\")\n        print(f\"- All entries for {target_date}: {'✅' if all_target else '❌'}\")\n        print(f\"Conversion {'✅' if dtype == 'datetime64[ns]' else '❌'}\")\n\n# Define datasets with conversion parameters\ndatasets = {\n    \"Dataset\": (\n        df, \"datetime\", \n        {\"format\": \"%d-%m-%Y %H:%M\", \"dayfirst\": True}\n    ),\n    \"Bronx Weather\": (\n        bronx_df, \"Date / Time\", \n        {\"errors\": \"coerce\", \"str.replace\": \" EDT\"}\n    ),\n    \"Manhattan Weather\": (\n        manhattan_df, \"Date / Time\", \n        {\"errors\": \"coerce\", \"str.replace\": \" EDT\"}\n    )\n}\n\n# Process and validate for July 24, 2021\nprocess_and_validate_datetime(datasets, \"24-07-2021\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:20:09.881680Z","iopub.execute_input":"2025-08-01T13:20:09.882435Z","iopub.status.idle":"2025-08-01T13:20:09.917734Z","shell.execute_reply.started":"2025-08-01T13:20:09.882399Z","shell.execute_reply":"2025-08-01T13:20:09.916845Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Analysis of Urban Heat Island (UHI) and Weather Data\n### 1. Overview\nThis report analyzes the dataset related to the Urban Heat Island (UHI) Index and weather conditions in the Bronx and Manhattan on **July 24, 2021**. The goal is to examine variations in UHI intensity and how meteorological factors may influence urban heating patterns.\n\n### 2. The Dataset (UHI Index) Summary\n- The dataset comprises *11,229 records*, covering geographical coordinates (`Longitude`, `Latitude`) and `UHI Index` values recorded within a single hour (**15:01 - 15:59**).\n- The *UHI Index ranges from 0.956 to 1.046*, with a mean of **1.00** and a small standard deviation (**0.016**), indicating minor variations across the dataset.\n- The `Longitude` (-73.99 to -73.87) and `Latitude` (40.75 to 40.85) values suggest the dataset is spatially constrained within a specific part of New York City.\n\n### 3. Bronx Weather Data Summary\n- `Temperature`: Ranges from **19.3°C to 28.4°C** (mean: **24.79°C**), showing significant daytime heating.\n- `Relative Humidity`: Varies between **39.6% and 88.2%** (mean: **54.45%**), indicating fluctuations in moisture levels throughout the day.\n- `Wind Speed`: Averages **2.35 m/s**, with a peak of **4.8 m/s**.\n- `Wind Direction`: Highly variable (**std: 92.28 degrees**), suggesting fluctuating wind patterns.\n- `Solar Flux`: Averages **387.87 W/m²**, with a maximum of **960 W/m²**, confirming strong solar radiation exposure.\n\n### 4. Manhattan Weather Data Summary\n- `Temperature`: Higher than in the Bronx (**mean: 25.20°C, max: 27.9°C**), suggesting increased heat retention.\n- `Relative Humidity`: Lower than in the Bronx (**mean: 49.40% vs. 54.45%**), indicating drier air conditions.\n- `Wind Speed`: Slightly weaker (**mean: 1.93 m/s**), which may contribute to increased heat retention.\n- `Wind Direction`: Mean value of **134.86 degrees**, slightly differing from the Bronx.\n- `Solar Flux`: Comparable to Bronx levels, but with a lower maximum (**840 W/m² vs. 960 W/m²**).\n\n### 5. Key Observations\n1. **Minimal UHI Index Variation**: The dataset shows **consistent heat intensity** across the recorded locations.\n2. **Manhattan Tends to be Warmer**: It has slightly higher **average temperatures** and **lower humidity**, which could amplify the UHI effect.\n3. **Lower Wind Speed in Manhattan**: Reduced airflow may hinder heat dissipation, contributing to heat buildup.\n4. **Solar Flux Levels are High and Consistent**: Suggesting strong solar radiation as a primary factor influencing heat accumulation.\n\n### 6. Conclusion\nThe findings highlight **how meteorological conditions influence urban heating**. Manhattan’s **higher temperatures, lower humidity, and weaker wind speeds** suggest a stronger UHI effect compared to the Bronx. Future analysis should explore spatial temperature variations, land cover impacts, and potential mitigation strategies to reduce urban heat retention.\n\n","metadata":{}},{"cell_type":"code","source":"print(\"\\nData Description:\")\nprint(df.describe())\n\nprint(\"\\nBronx Weather Data Description:\")\nprint(bronx_df.describe())\n\nprint(\"\\nManhattan Weather Data Description:\")\nprint(manhattan_df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:20:21.180839Z","iopub.execute_input":"2025-08-01T13:20:21.181194Z","iopub.status.idle":"2025-08-01T13:20:21.233415Z","shell.execute_reply.started":"2025-08-01T13:20:21.181165Z","shell.execute_reply":"2025-08-01T13:20:21.232386Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_distributions(df, dataset_name):\n    \"\"\"\n    Plots histogram distributions for all numerical columns in a dataset.\n\n    Parameters:\n    df (pd.DataFrame): The dataset to analyze.\n    dataset_name (str): Name of the dataset for labeling plots.\n    \"\"\"\n    num_cols = df.select_dtypes(include=['number']).columns  # Select only numerical columns\n\n    for col in num_cols:\n        plt.figure(figsize=(8, 5))\n        sns.histplot(df[col], kde=True)\n        plt.title(f\"{dataset_name} - {col} Distribution\")\n        plt.xlabel(col)\n        plt.ylabel(\"Frequency\")\n        plt.show()\n\n# Example usage:\nplot_distributions(bronx_df, \"Bronx Weather Data\")\nplot_distributions(manhattan_df, \"Manhattan Weather Data\")\nplot_distributions(df, \"The Dataset\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_boxplots(data, title=\"Boxplot of Weather Variables\"):\n    \"\"\"\n    Generates a boxplot for all numerical columns in a given dataset.\n    \n    Parameters:\n    - data: pandas DataFrame\n    - title: str, title for the plot (default: \"Boxplot of Weather Variables\")\n    \"\"\"\n    # Select only numerical columns\n    numeric_cols = data.select_dtypes(include=['number']).columns\n    \n    if numeric_cols.empty:\n        print(\"No numerical columns found in the dataset.\")\n        return\n    \n    plt.figure(figsize=(10, 6))\n    sns.boxplot(data=data[numeric_cols])\n    plt.xticks(rotation=45)  # Rotate labels if needed\n    plt.title(title)\n    plt.show()\n\n# Example usage for Bronx and Manhattan weather data\nplot_boxplots(bronx_df, title=\"Boxplot of Bronx Weather Variables\")\nplot_boxplots(manhattan_df, title=\"Boxplot of Manhattan Weather Variables\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To ensure accurate dataset merging, I assigned boroughs (Bronx or Manhattan) based on **official NYC borough boundary data** rather than relying on a simple latitude threshold. I achieved this by mapping the **longitude and latitude coordinates** of each data point to the **Borough Boundaries (Clipped to Shoreline)** shapefile from NYC Open Data.  \n\nThis location column is a **temporary feature** used **only** to facilitate dataset merging—such as integrating weather and building footprint data with the UHI training dataset. It will be **removed before model training** to comply with competition rules, ensuring that raw coordinates are not used as predictive features.  \n\nI verified that this approach is **within the competition guidelines**, as long as the location column is excluded from the final model.  \n\n**Source:**  \nNYC Open Data - [Borough Boundaries (Clipped to Shoreline)](https://www.nyc.gov/site/planning/data-maps/open-data/districts-download-metadata.page)  ","metadata":{}},{"cell_type":"code","source":"# File path to NYC borough boundaries shapefile\nborough_shapefile = \"/kaggle/input/ey-challenge/nybb_25a/nybb.shp\"\n\ndef assign_boroughs(df, borough_shapefile):\n    \"\"\"\n    Assigns boroughs to a DataFrame based on longitude and latitude.\n    \n    Parameters:\n    - df (pd.DataFrame): DataFrame with 'Longitude' and 'Latitude' columns.\n    - borough_shapefile (str): Path to the NYC borough boundaries shapefile.\n\n    Returns:\n    - df (pd.DataFrame): Updated DataFrame with a new 'Location' column.\n    \"\"\"\n    # Load NYC borough boundaries\n    boroughs = gpd.read_file(borough_shapefile)\n\n    # Reproject boroughs to match the CRS of the DataFrame\n    boroughs = boroughs.to_crs(epsg=4326)\n\n    # Convert df into a GeoDataFrame\n    geometry = [Point(xy) for xy in zip(df[\"Longitude\"], df[\"Latitude\"])]\n    gdf = gpd.GeoDataFrame(df, geometry=geometry, crs=\"EPSG:4326\")\n\n    # Perform spatial join to assign boroughs\n    gdf = gpd.sjoin(gdf, boroughs, how=\"left\", predicate=\"within\")\n\n    # Inspect columns to identify the correct column name for boroughs\n    print(gdf.columns)\n\n    # Add the borough information back to the original DataFrame\n    df[\"Location\"] = gdf[\"BoroName\"]  # Use the actual column name from the shapefile\n\n    return df\n\n# Apply function to the dataset\ndf = assign_boroughs(df, borough_shapefile)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:20:32.133537Z","iopub.execute_input":"2025-08-01T13:20:32.133984Z","iopub.status.idle":"2025-08-01T13:20:33.020726Z","shell.execute_reply.started":"2025-08-01T13:20:32.133939Z","shell.execute_reply":"2025-08-01T13:20:33.019410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df['Location'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:20:36.057247Z","iopub.execute_input":"2025-08-01T13:20:36.057604Z","iopub.status.idle":"2025-08-01T13:20:36.065169Z","shell.execute_reply.started":"2025-08-01T13:20:36.057574Z","shell.execute_reply":"2025-08-01T13:20:36.063928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To understand the temporal resolution of the dataset, I first checked the number of unique timestamps in each subset. The **train data** contains **59 unique timestamps**, recorded at **one-minute intervals** between **3 PM and 4 PM**. In contrast, the **Bronx and Manhattan weather datasets** have **169 unique timestamps each**, recorded at **five-minute intervals** from **6 AM to 8 PM**.  \n\n### Unique Timestamps:  \n- **The Data:** 59 (1-minute interval, 3 PM - 4 PM)  \n- **Bronx Data:** 169 (5-minute interval, 6 AM - 8 PM)  \n- **Manhattan Data:** 169 (5-minute interval, 6 AM - 8 PM)  \n\n### Weather Data Time Intervals:  \n- **Bronx:** July 24, 2021, **06:00 AM – 08:00 PM**  \n- **Manhattan:** July 24, 2021, **06:00 AM – 08:00 PM**  \n\nThis gives me a clear understanding of how the timestamps are structured across datasets, which is crucial for aligning temporal features in my analysis.  \n","metadata":{}},{"cell_type":"code","source":"print(\"Unique timestamps in The dataset:\", df[\"datetime\"].nunique())\nprint(\"Unique timestamps in Bronx:\", bronx_df[\"Date / Time\"].nunique())\nprint(\"Unique timestamps in Manhattan:\", manhattan_df[\"Date / Time\"].nunique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:20:39.792993Z","iopub.execute_input":"2025-08-01T13:20:39.793333Z","iopub.status.idle":"2025-08-01T13:20:39.802103Z","shell.execute_reply.started":"2025-08-01T13:20:39.793308Z","shell.execute_reply":"2025-08-01T13:20:39.800950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bronx_min_time = bronx_df[\"Date / Time\"].min()\nbronx_max_time = bronx_df[\"Date / Time\"].max()\n\nmanhattan_min_time = manhattan_df[\"Date / Time\"].min()\nmanhattan_max_time = manhattan_df[\"Date / Time\"].max()\n\n# Print the time intervals\nprint(\"Bronx Weather Data Time Interval:\")\nprint(f\"From: {bronx_min_time} To: {bronx_max_time}\")\n\nprint(\"\\nManhattan Weather Data Time Interval:\")\nprint(f\"From: {manhattan_min_time} To: {manhattan_max_time}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:20:58.569366Z","iopub.execute_input":"2025-08-01T13:20:58.569783Z","iopub.status.idle":"2025-08-01T13:20:58.577974Z","shell.execute_reply.started":"2025-08-01T13:20:58.569750Z","shell.execute_reply":"2025-08-01T13:20:58.576835Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Trend-Based Weather Feature Engineering for UHI Prediction\nTo resolve the temporal mismatch between the weather data (`6am – 8pm`) and the UHI training data (captured between `3pm – 4pm`), I avoided restricting the weather inputs to a fixed hour. Instead, I designed a feature engineering approach that focuses on broader daytime weather trends. By capturing how heat accumulates throughout the day—rather than isolating a single time slice—the model can learn the underlying patterns that lead to UHI formation. This strategy emphasizes overall weather behavior leading up to UHI observations, offering a more robust and context-aware understanding of urban heat dynamics.\n\n### What I Did\n1. **Time-Aware Anchoring**\n\n   * Calculated `mins_from_3pm` to model temporal distance from peak UHI observations.\n\n2. **Trend Extraction & Temporal Smoothing**\n\n   * Computed 1-hour and 3-hour moving averages for surface temperature and humidity to capture short- and mid-term trends.\n   * Applied `.shift(1)` to all moving metrics to avoid any look-ahead leakage.\n\n3. **Cumulative Energy Indicators**\n\n   * Created `solar_accumulation_6am`, tracking cumulative solar flux since morning to reflect how much heat energy may have been absorbed by the urban surface throughout the day.\n\n4. **Short-Term Change Features**\n\n   * Included delta features such as 30-minute temperature change and 1-hour humidity change to detect quick transitions that may influence heat buildup.\n\n5. **Hourly Aggregations**\n\n   * Resampled raw weather data into hourly windows, computing summary statistics (mean, min, max, std) for temperature, humidity, wind speed/direction, and solar flux.\n   * These provide a higher-level snapshot of atmospheric variability without tying the model to a single timestamp.\n\n6. **Cyclical Time Encoding**\n\n   * Encoded time of day using sine and cosine transformations to preserve diurnal cycles while maintaining continuity across hours.\n\n7. **Nearest-Time Merging Strategy**\n\n   * Used `merge_asof()` to match each UHI data point with the nearest weather profile, emphasizing trend alignment over perfect timestamp synchronization.\n\n### Rationale\nRather than relying on fixed-hour readings that may not fully represent the conditions leading to UHI formation, this approach captures the **underlying dynamics of daytime heat accumulation**. By anchoring features to 3 PM but drawing trends from the entire day:\n\n* The model learns how temperature and solar behavior **develop and shift**, not just their values at a single moment.\n* It accounts for **both gradual trends and rapid transitions**, offering a richer understanding of heat buildup.\n* It reduces overfitting risk by summarizing behavior, not memorizing time-specific noise.\n\nThis method also preserves spatial and temporal flexibility, especially useful given the uneven timestamp coverage in the original weather dataset.\n\n### Conclusion\n\nThis trend-focused feature engineering pipeline enables the model to learn from **how heat forms**, not just **when it’s observed**. By incorporating temporal distance, smoothing, cumulative energy, and short-term changes:\n\n* The model becomes sensitive to the **physical processes driving UHI**, improving generalizability.\n* The dataset stays compatible with test-time conditions where exact timestamps may be unavailable.\n* It strikes a balance between interpretability, robustness, and domain relevance—forming a strong foundation for UHI prediction.","metadata":{}},{"cell_type":"code","source":"def preprocess_weather_data(weather_df):\n    \"\"\"Applies smoothing, aggregations, time differences to weather data.\"\"\"\n    weather_df = weather_df.copy()\n    weather_df['Date / Time'] = pd.to_datetime(weather_df['Date / Time'])\n    weather_df.set_index('Date / Time', inplace=True)\n\n    # Fill missing values before feature engineering\n    weather_df = weather_df.ffill().bfill()\n\n    # --- MOVING AVERAGES (FIXED) --- \n    weather_df['Temp_1hr_MA'] = weather_df['Air Temp at Surface [degC]'].rolling(window=60, min_periods=1).mean().shift(1)\n    weather_df['Temp_3hr_MA'] = weather_df['Air Temp at Surface [degC]'].rolling(window=180, min_periods=1).mean().shift(1)\n    weather_df['Humidity_1hr_MA'] = weather_df['Relative Humidity [percent]'].rolling(window=60, min_periods=1).mean().shift(1)\n    \n    # --- AGGREGATIONS (SAFE) ---\n    weather_hourly = weather_df.resample('h').agg({\n        'Air Temp at Surface [degC]': ['mean', 'min', 'max', 'std'],\n        'Relative Humidity [percent]': ['mean', 'min', 'max', 'std'],\n        'Avg Wind Speed [m/s]': ['mean'],\n        'Wind Direction [degrees]': ['mean'],\n        'Solar Flux [W/m^2]': ['mean']\n    })\n    weather_hourly.columns = ['_'.join(col).strip() for col in weather_hourly.columns]\n    weather_hourly = weather_hourly.ffill().bfill()\n\n    # --- TIME DIFFERENCES (FIXED) ---\n    weather_df['Temp_Change_30min'] = weather_df['Air Temp at Surface [degC]'].diff(periods=30).shift(1)\n    weather_df['Humidity_Change_1hr'] = weather_df['Relative Humidity [percent]'].diff(periods=60).shift(1)\n\n    # --- TIME FEATURES ---\n    weather_df['mins_from_3pm'] = (weather_df.index.hour - 15) * 60 + weather_df.index.minute\n    weather_df['solar_accumulation_6am'] = weather_df['Solar Flux [W/m^2]'].rolling('6h').sum().shift(1)\n\n    # Merge processed weather data\n    weather_df = weather_df.merge(weather_hourly, left_index=True, right_index=True, how='left')\n    weather_df.reset_index(inplace=True)\n\n    return weather_df\n    \ndef upcycle_weather_for_training(df, bronx_df, manhattan_df):\n    \"\"\"Upsamples weather data and merges it with training data (11229 rows).\"\"\"\n    # Preprocess weather data (now with safe features)\n    bronx_weather = preprocess_weather_data(bronx_df)\n    manhattan_weather = preprocess_weather_data(manhattan_df)\n\n    # Fill missing values before merging\n    bronx_weather = bronx_weather.ffill().bfill()\n    manhattan_weather = manhattan_weather.ffill().bfill()\n\n    # MERGING (SAFE WITH NEAREST TIMESTAMP) \n    training_bronx = df[df['Location'] == 'Bronx'].copy().sort_values('datetime')\n    training_manhattan = df[df['Location'] == 'Manhattan'].copy().sort_values('datetime')\n    \n    bronx_weather.sort_values('Date / Time', inplace=True)\n    manhattan_weather.sort_values('Date / Time', inplace=True)\n\n    training_bronx_merged = pd.merge_asof(\n        training_bronx, bronx_weather,\n        left_on='datetime', right_on='Date / Time',\n        direction='nearest'\n    )\n\n    training_manhattan_merged = pd.merge_asof(\n        training_manhattan, manhattan_weather,\n        left_on='datetime', right_on='Date / Time',\n        direction='nearest'\n    )\n\n    # Combine both locations\n    final_training = pd.concat([training_bronx_merged, training_manhattan_merged])\n    \n    # Check and drop redundant datetime column \n    if 'Date / Time' in final_training.columns:\n        # Compare dates (ignore time differences)\n        dates_match = (\n        pd.to_datetime(final_training['datetime']).dt.floor('D')\n        .equals(pd.to_datetime(final_training['Date / Time']).dt.floor('D'))\n    )\n        if dates_match:\n            final_training.drop('Date / Time', axis=1, inplace=True)\n            print(\"Dropped 'Date / Time' (dates match, time ignored)\")\n        else:\n            print(\"Warning: Dates differ—keeping both columns\")\n    \n    return final_training.sort_index().reset_index(drop=True)\n\n# Upcycle and merge weather data with training dataset\nfinal_df = upcycle_weather_for_training(df, bronx_df, manhattan_df)\n\n# Verify shapes\nprint(\"Final data shape:\", final_df.shape) # Should be (11229, 28)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:21:07.585267Z","iopub.execute_input":"2025-08-01T13:21:07.585626Z","iopub.status.idle":"2025-08-01T13:21:07.701115Z","shell.execute_reply.started":"2025-08-01T13:21:07.585600Z","shell.execute_reply":"2025-08-01T13:21:07.699932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:22:52.185954Z","iopub.execute_input":"2025-08-01T13:22:52.186367Z","iopub.status.idle":"2025-08-01T13:22:52.201173Z","shell.execute_reply.started":"2025-08-01T13:22:52.186336Z","shell.execute_reply":"2025-08-01T13:22:52.200111Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_distributions(final_df, \"The Upcycycled Dataset\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_boxplots(final_df, title=\"Boxplot of Upcycycled Weather Variables\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Investigating Duplicates","metadata":{}},{"cell_type":"code","source":"def investigate_duplicates(df, id_columns=None, report_only=False, sample_size=3):\n    \"\"\"\n    Universal duplicate detective that:\n    1. Identifies ALL duplicate types\n    2. Classifies their severity\n    3. Recommends solutions\n    4. Returns cleaned data (optional)\n    \n    Usage: Call after any processing step:\n    - after upcycling: investigate_duplicates(final_df, ['datetime', 'Location'])\n    - after feature engineering: investigate_duplicates(df_engineered)\n    \"\"\"\n    \n    # Set default ID columns if not specified\n    if id_columns is None:\n        id_columns = df.columns.tolist()\n    \n    # Detect all duplicate types\n    analysis = {\n        'exact_duplicates': df[df.duplicated(keep=False)],\n        'key_duplicates': df[df.duplicated(subset=id_columns, keep=False)],\n        'conflict_duplicates': pd.DataFrame()\n    }\n    \n    # Find records with same keys but different other values\n    if len(id_columns) < len(df.columns):\n        key_groups = df.groupby(id_columns)\n        conflict_mask = key_groups.transform('nunique').gt(1).any(axis=1)\n        analysis['conflict_duplicates'] = df[conflict_mask]\n    \n    # Generate automated report\n    report = {\n        'stage': 'Current DataFrame',\n        'total_rows': len(df),\n        'exact_duplicates_count': len(analysis['exact_duplicates']),\n        'key_duplicates_count': len(analysis['key_duplicates']),\n        'conflict_duplicates_count': len(analysis['conflict_duplicates']),\n        'exact_duplicates_sample': analysis['exact_duplicates'].head(sample_size),\n        'key_duplicates_sample': analysis['key_duplicates'].head(sample_size),\n        'conflict_duplicates_sample': analysis['conflict_duplicates'].head(sample_size),\n        'recommendation': None\n    }\n    \n    # Determine recommendation\n    if report['exact_duplicates_count'] > 0:\n        report['recommendation'] = \"✅ SAFE TO DROP - These are true duplicates\"\n    elif report['conflict_duplicates_count'] > 0:\n        report['recommendation'] = (f\"🚨 INVESTIGATE - {len(analysis['conflict_duplicates'])} records have \"\n                                  f\"conflicting values for the same keys ({id_columns})\")\n    elif report['key_duplicates_count'] > 0:\n        report['recommendation'] = \"⚠️ REVIEW - Duplicate keys but consistent values\"\n    else:\n        report['recommendation'] = \"✅ No problematic duplicates found\"\n    \n    # Print human-readable report\n    print(f\"\\n🔍 Duplicate Analysis Report - {report['stage']}\")\n    print(\"-\"*50)\n    print(f\"Total rows: {report['total_rows']}\")\n    print(f\"Exact duplicates: {report['exact_duplicates_count']}\")\n    print(f\"Key duplicates: {report['key_duplicates_count']}\")\n    print(f\"Conflicts: {report['conflict_duplicates_count']}\")\n    print(\"\\nRecommendation:\", report['recommendation'])\n    \n    if not report_only and (report['exact_duplicates_count'] > 0):\n        print(\"\\n🔧 Auto-cleaning exact duplicates...\")\n        df = df.drop_duplicates()\n        return df\n    elif not report_only and (report['key_duplicates_count'] > 0 and report['conflict_duplicates_count'] == 0):\n        print(\"\\n🔧 Auto-cleaning non-conflicting key duplicates...\")\n        df = df.drop_duplicates(subset=id_columns)\n        return df\n    \n    return df if not report_only else report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T16:51:57.789652Z","iopub.execute_input":"2025-07-31T16:51:57.790056Z","iopub.status.idle":"2025-07-31T16:51:57.801185Z","shell.execute_reply.started":"2025-07-31T16:51:57.790026Z","shell.execute_reply":"2025-07-31T16:51:57.799837Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"investigate_duplicates(final_df, id_columns=['datetime', 'Longitude', 'Latitude'], report_only=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T16:52:01.390425Z","iopub.execute_input":"2025-07-31T16:52:01.390898Z","iopub.status.idle":"2025-07-31T16:52:01.461852Z","shell.execute_reply.started":"2025-07-31T16:52:01.390855Z","shell.execute_reply":"2025-07-31T16:52:01.460572Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Move 'UHI Index' to the end\ncols = final_df.columns.tolist()  # Get all column names as a list\ncols.remove(\"UHI Index\")  # Remove 'UHI Index' from its current position\ncols.append(\"UHI Index\")  # Append 'UHI Index' at the end\n\n# Reorder the dataframe\nfinal_df = final_df[cols]\n\n# Display the new column order\nprint(final_df.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:27:02.953556Z","iopub.execute_input":"2025-08-01T13:27:02.954015Z","iopub.status.idle":"2025-08-01T13:27:02.973218Z","shell.execute_reply.started":"2025-08-01T13:27:02.953976Z","shell.execute_reply":"2025-08-01T13:27:02.972162Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Handling Missing Values in Weather Data \n\nAfter preprocessing, I verified the completeness of the weather dataset and found that there were **no missing values** in the key weather features. This ensured that all 11,229 training observations were fully populated with the engineered weather features—eliminating the need for additional imputation or data correction at this stage.","metadata":{}},{"cell_type":"code","source":"# Count missing values per column\nmissing_counts = final_df.isnull().sum()\nmissing_percent = (missing_counts / len(final_df)) * 100\nmissing_percent","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:27:12.354867Z","iopub.execute_input":"2025-08-01T13:27:12.355262Z","iopub.status.idle":"2025-08-01T13:27:12.365596Z","shell.execute_reply.started":"2025-08-01T13:27:12.355227Z","shell.execute_reply":"2025-08-01T13:27:12.364585Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### **Verification of Weather Data Integration**\n\nTo validate the integration of weather features (`Air Temp at Surface [°C]`, `Relative Humidity [%]`, `Avg Wind Speed [m/s]`, `Solar Flux [W/m²]`, and `Wind Direction [°]`) with UHI observations, I reviewed the summary statistics after expanding the time window used during interpolation. Instead of restricting the weather data to just the 3–4 p.m. timeframe, I utilized a broader temporal span from `6am - 8 pm`, to enrich the dataset with more complete atmospheric context.\n\n\n### **Key Findings:**\n\n1. **Expanded Temporal Context:**\n\n   * The final merged dataset reflects a wider range of diurnal weather patterns, capturing not only peak afternoon conditions but also transitional periods across morning and evening.\n   * For instance, the **mean air temperature is 27.14 °C**, slightly higher than Bronx (24.79 °C) and Manhattan (25.20 °C), due to the inclusion of warmer afternoon hours.\n\n2. **Relative Humidity Adjustments:**\n\n   * **Humidity levels stabilized at a mean of 46.39%**, lower than Bronx (54.45%) and Manhattan (49.40%), likely due to afternoon readings outweighing early morning humidity spikes.\n   * This broader range better captures day-long variability in moisture levels, improving generalizability.\n\n3. **Wind Speed & Direction Variation:**\n\n   * The **average wind speed is 3.09 m/s**, higher than in the station-level stats, suggesting increased wind activity during daytime hours.\n   * **Wind direction converged at 159.37°**, reflecting directional trends over the full course of the day.\n\n4. **Solar Flux Trends:**\n\n   * **Mean solar flux rose to 452.17 W/m²**, consistent with the cumulative effect of sunlight from late morning through late afternoon.\n   * Minimum and maximum values (128–725 W/m²) are within reasonable bounds, with no anomalous jumps—indicating that interpolation across the full day did not distort solar exposure patterns.\n\n### **Conclusion:**\n\nBy interpolating across a broader time span (`6am - 8pm`), the final weather dataset provides a more complete representation of daily atmospheric dynamics. This decision trades temporal specificity for a richer signal, allowing the UHI model to learn from a fuller picture of environmental influences rather than a narrow snapshot. The integrity of core weather features was preserved throughout, making the dataset robust for downstream modeling.\n","metadata":{}},{"cell_type":"code","source":"print(\"\\nFinal Training Data Description:\")\n# (Optional) Check summary statistics to ensure distributions haven't changed drastically\nprint(final_df.describe())\n\nprint(\"Unique timestamps in Final Training Data:\", final_df[\"datetime\"].nunique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:28:16.689547Z","iopub.execute_input":"2025-08-01T13:28:16.689968Z","iopub.status.idle":"2025-08-01T13:28:16.758720Z","shell.execute_reply.started":"2025-08-01T13:28:16.689936Z","shell.execute_reply":"2025-08-01T13:28:16.757673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initial split\nX = final_df.drop(columns=[\"UHI Index\"])\ny = final_df[\"UHI Index\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\n# Combine training set for easier processing\ntrain_df = X_train.copy()\ntrain_df[\"UHI Index\"] = y_train\n\ntest_df = X_test.copy()\ntest_df[\"UHI Index\"] = y_test  # only if you need it in one block too","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:28:24.946695Z","iopub.execute_input":"2025-08-01T13:28:24.947102Z","iopub.status.idle":"2025-08-01T13:28:24.961260Z","shell.execute_reply.started":"2025-08-01T13:28:24.947073Z","shell.execute_reply":"2025-08-01T13:28:24.960224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df_check = investigate_duplicates(\n    train_df, id_columns=['datetime', 'Longitude', 'Latitude'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T16:52:20.893085Z","iopub.execute_input":"2025-07-31T16:52:20.893434Z","iopub.status.idle":"2025-07-31T16:52:20.953106Z","shell.execute_reply.started":"2025-07-31T16:52:20.893410Z","shell.execute_reply":"2025-07-31T16:52:20.952050Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Correlation Analysis of UHI Predictors\n\nThis analysis identifies which environmental features show the strongest relationships with Urban Heat Island (UHI) intensity. These insights help guide feature selection and refinement for modeling.\n\n###  Key Patterns\n#### 1. **Spatial & Temporal Context**\n\n* **Latitude (0.45) and Longitude (0.39)** had the strongest correlations with UHI — not surprising, given geography’s link to land use and density.\n\n  * These are excluded in modeling, so **proxy variables** like land cover or building density will be essential.\n\n* **Short-term weather trends matter**:\n\n  * **1-hour temperature and humidity averages** (e.g., 0.21 and 0.37) positively correlate with UHI.\n  * Surprisingly, **3-hour temperature trend** shows a *negative* correlation (−0.37), possibly reflecting cooling effects or data quirks that need further validation.\n\n* **Solar accumulation by morning (0.33)** shows a clear positive link — areas receiving more early-day solar radiation tend to retain more heat.\n\n#### 2. **Temperature & Humidity**\n\n* **Instantaneous temperature metrics** (raw, min/max/mean) show mild positive correlations (\\~0.25–0.37).\n* **Relative humidity** shows **negative correlations** across raw and smoothed values (e.g., −0.22 to −0.37), supporting the idea that **drier air = stronger UHI**.\n* **Humidity variability (std)** has a **positive correlation (0.32)**, possibly pointing to the role of fluctuating moisture in heat dynamics.\n\n#### 3. **Wind & Solar Flux**\n\n* **Wind direction mean (−0.37)** had a stronger signal than raw wind direction, hinting that **consistent breezes** may help reduce UHI.\n* **Wind speed mean (0.30)** showed a positive correlation — possibly reflecting urban conditions where stronger winds don’t always cool.\n* **Solar flux** had weak correlations overall, suggesting its direct effect on UHI may be non-linear or moderated by other variables.\n\n\n### Insights & Priorities\n* **Top predictive groups**:\n  * Temporal averages of **temperature & humidity**\n  * **Cumulative solar exposure**\n  * **Wind direction patterns**\n\n* **Counterintuitive signals**:\n  * **Negative 3-hour temperature trends** raise questions — worth examining lags or feature engineering choices.\n  * **Wind speed’s positive link** challenges assumptions and may require deeper spatial context (e.g., urban vs. coastal flow).","metadata":{}},{"cell_type":"code","source":"# Display correlation of all features against 'UHI Index'\ncorrelations = train_df.corr(numeric_only=True)['UHI Index'].drop('UHI Index')\ncorrelations_sorted = correlations.sort_values(ascending=False)\n\nprint(\"Correlation of features with UHI Index:\\n\")\nprint(correlations_sorted)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:28:32.225267Z","iopub.execute_input":"2025-08-01T13:28:32.225600Z","iopub.status.idle":"2025-08-01T13:28:32.253829Z","shell.execute_reply.started":"2025-08-01T13:28:32.225576Z","shell.execute_reply":"2025-08-01T13:28:32.252528Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Feature Engineering: Weather Data Transformations**  \n\nTo enhance our model's predictive power, we leveraged domain knowledge from the weather data documentation to engineer meaningful features. These transformations capture environmental factors affecting Urban Heat Islands (UHI more effectively than raw weather variables.\n\n**1.  Computing Heat Index (Apparent Temperature)**  \nThe heat index (HI estimates how hot it *feels* by combining air temperature and relative humidity. This is crucial because UHI isn't just about air temperature—it’s about perceived heat.  \n\n**NOAA Approximation:**  \nThe National Oceanic and Atmospheric Administration (NOAA) provides an empirical formula for computing the heat index:  \n\nHI = c₁ + c₂T + c₃RH + c₄T⋅RH + c₅T² + c₆RH² + c₇T²⋅RH + c₈T⋅RH² + c₉T²⋅RH²\n\nwhere:  \n- T = Air Temperature (°C)  \n- RH = Relative Humidity (%)  \n- The coefficients (c₁ to c₉) are empirically derived constants.\n\n**How the Formula Works:**  \n- The formula starts with a baseline temperature (c₁) and adds weighted contributions from temperature and humidity.  \n- Interaction terms like (T⋅RH) and (T²⋅RH²) help capture nonlinear effects, meaning the perceived temperature doesn’t just increase linearly with heat and humidity—it escalates.  \n- Higher humidity levels reduce the body’s ability to cool through sweat evaporation, making it feel even hotter.  ","metadata":{}},{"cell_type":"markdown","source":"**2. Transforming Wind Direction into U/V Components**\n\nWind direction is a circular variable, meaning 0° and 360° represent the same direction. Using raw wind direction in models can lead to incorrect patterns. Instead, we break it down into U/V wind components, which are easier to interpret.  \n\n**Formula for U/V Wind Components:**  \nWe decompose wind into two perpendicular vectors using trigonometry:\n\nU = -Wind Speed × sin(Wind Direction)\n\nV = -Wind Speed × cos(Wind Direction)\n\nwhere:  \n- U represents east-west (zonal) wind flow  \n- V represents north-south (meridional) wind flow  \n- Negative signs are used to align with meteorological conventions.\n\n**Why This Works:**  \n- Wind speed alone doesn’t describe direction effectively.  \n- Splitting it into two components (U, V allows the model to capture directional effects.  \n- These components can interact with UHI, helping to determine whether wind cools or amplifies urban heat.  ","metadata":{}},{"cell_type":"markdown","source":"**3. Explore Solar Flux & Building Footprint Interactions**\nThe documentation suggests that solar flux alone isn’t enough—it interacts with building footprint and height to determine shading effects.\n\n\n\nThis initial feature engineering process improves our dataset by making variables more interpretable and aligned with physical processes affecting UHI. Further refinements will follow as we integrate additional datasets like Landsat, Sentinel-2, and building footprints","metadata":{}},{"cell_type":"code","source":"# NOAA Heat Index formula coefficients\ndef compute_heat_index(T, RH):\n    return (\n        -42.379\n        + 2.04901523 * T\n        + 10.14333127 * RH\n        - 0.22475541 * T * RH\n        - 0.00683783 * T**2\n        - 0.05481717 * RH**2\n        + 0.00122874 * T**2 * RH\n        + 0.00085282 * T * RH**2\n        - 0.00000199 * T**2 * RH**2\n    )\n\ndef preprocess_weather_features(df):\n    T = df[\"Air Temp at Surface [degC]\"]\n    RH = df[\"Relative Humidity [percent]\"]\n\n    df[\"Heat Index\"] = compute_heat_index(T, RH)\n    df[\"Wind Direction (radians)\"] = np.radians(df[\"Wind Direction [degrees]\"])\n    df[\"Wind U\"] = -df[\"Avg Wind Speed [m/s]\"] * np.sin(df[\"Wind Direction (radians)\"])\n    df[\"Wind V\"] = -df[\"Avg Wind Speed [m/s]\"] * np.cos(df[\"Wind Direction (radians)\"])\n    df = df.drop(columns=[\"Wind Direction (radians)\"])\n    return df\n\n# Apply to training data\nfinal_train = preprocess_weather_features(train_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:29:06.912884Z","iopub.execute_input":"2025-08-01T13:29:06.913218Z","iopub.status.idle":"2025-08-01T13:29:06.944560Z","shell.execute_reply.started":"2025-08-01T13:29:06.913196Z","shell.execute_reply":"2025-08-01T13:29:06.943616Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Feature Analysis: Correlation with UHI Index**  \n### **Key Insights**  \nThe latest correlation analysis reveals how engineered features relate to Urban Heat Island (UHI) intensity. Below are the standout patterns and their implications for modeling:  \n\n### **1. Heat Index: Weak but Meaningful Negative Link (−0.23)**  \n- **What it means**: Higher heat index values (combining temperature and humidity) slightly correlate with lower UHI intensity.  \n- **Why it matters**: This suggests that humid heat may mitigate UHI in some areas—possibly due to evaporative cooling from vegetation or water bodies.  \n\n### **2. Wind U Component: Moderate Cooling Signal (−0.18)**  \n- **What it means**: Westward winds (negative U values) associate with higher UHI, implying eastward winds may help disperse heat.  \n- **Why it matters**: Urban geometry (e.g., building alignment) likely influences how winds redistribute heat.   \n\n### **3. Wind V Component: Minimal Direct Impact (−0.09)**  \n- **What it means**: North-south winds show almost no correlation with UHI, unlike their east-west (U) counterparts.  \n- **Why it matters**: This hints that local topography or infrastructure may block north-south airflow, limiting its role in heat dispersion.   \n\n### **4. Solar Flux: Neutral Influence (0.004)**  \n- **What it means**: Direct solar radiation has no linear relationship with UHI in this dataset.  \n- **Why it matters**: Urban heat retention seems driven more by surface properties (e.g., materials, shading) than raw solar input.  \n\n### **5. Wind and Heat Index: Strong Coupling**  \n- **Key relationships**:  \n  - *Wind U ↔ Heat Index (0.44)*: Westward winds coincide with higher perceived temperatures.  \n  - *Wind V ↔ Heat Index (0.64)*: North-south winds correlate even more strongly with humid heat.  \n- **Implication**: Winds may transport humid air masses, indirectly affecting UHI by altering local humidity.  ","metadata":{}},{"cell_type":"code","source":"# Compute correlation matrix\ncorr_matrix = final_train[[\"UHI Index\", \"Heat Index\", \"Wind U\", \"Wind V\", \"Solar Flux [W/m^2]\"]].corr()\n\n# Plot heatmap\nplt.figure(figsize=(8, 5))\nsns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\nplt.title(\"Correlation Matrix of New Features with UHI Index\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:29:12.192406Z","iopub.execute_input":"2025-08-01T13:29:12.192812Z","iopub.status.idle":"2025-08-01T13:29:12.752588Z","shell.execute_reply.started":"2025-08-01T13:29:12.192779Z","shell.execute_reply":"2025-08-01T13:29:12.751605Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Move 'UHI Index' to the end\ncols = final_train.columns.tolist()\ncols.remove(\"UHI Index\")\ncols.append(\"UHI Index\")\nfinal_train = final_train[cols]\nprint(final_train.info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:33:29.341486Z","iopub.execute_input":"2025-08-01T13:33:29.341987Z","iopub.status.idle":"2025-08-01T13:33:29.361848Z","shell.execute_reply.started":"2025-08-01T13:33:29.341948Z","shell.execute_reply":"2025-08-01T13:33:29.360655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"investigate_duplicates(\n    final_train, id_columns=['datetime', 'Longitude', 'Latitude'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-31T16:52:44.638152Z","iopub.execute_input":"2025-07-31T16:52:44.638689Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_train.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Reduction Using Correlation Thresholding\n\nTo streamline the feature set and reduce redundancy—especially important for linear models like Ridge Regression where multicollinearity can distort coefficient estimates—I applied **correlation-based feature selection**. This time, I incorporated a set of **domain-relevant features** (`Air Temp at Surface degC`, `Solar Flux W/m^2`, etc.) that were manually protected from removal due to their strong physical relevance to UHI, even if correlated.\n\nInstead of treating every feature equally, I allowed the algorithm to search for an **optimal correlation threshold** while **preserving high-value predictors**. This process identified **10 features** at a **threshold of 0.70**, balancing simplicity and predictive performance.\n\n### Key Insights from the Results\n\nAt the **0.70 correlation threshold**, the **average R² across Ridge, Random Forest, and Gradient Boosting models** was **0.180**. This indicates that the selected features collectively explain \\~18% of the variation in the Urban Heat Island (UHI) Index.\n\n* **Ridge Regression** underperformed slightly (**R² = 0.160**), as expected for a model sensitive to residual multicollinearity.\n* **Random Forest and Gradient Boosting** models performed better (**R² ≈ 0.190**), suggesting that tree-based learners extract more nuanced interactions even from a small, optimized set.\n* Raising the threshold to allow **1 extra feature** (11 total) provided *no gain* in performance, confirming that the **10-feature set is optimal** under current conditions.\n\n### The Selected Features\n\nThe final retained features cover core UHI-relevant domains:\n\n* **Air Temperature** (`Air Temp at Surface degC`, `Temp_1hr_MA`, `Temp_3hr_MA`, `Temp_Change_30min`)\n* **Solar Exposure** (`Solar Flux W/m^2`, `solar_accumulation_6am`)\n* **Wind Metrics** (`Avg Wind Speed m/s`, `Wind Direction degrees`, `Wind V`)\n* **Temporal Signal** (`mins_from_3pm`)\n\nThese features reflect the thermal dynamics, wind dispersal, and solar forcing mechanisms that underlie UHI intensity—consistent with both literature and intuition.\n\n### Why Some Features Were Manually Protected\n\nCertain features were **intentionally excluded from correlation-based removal** because they encode critical environmental inputs (e.g., cumulative solar energy or diurnal thermal lag) that are **likely non-redundant in practice**, even if statistically correlated. Given that tree-based models can handle feature overlap gracefully, preserving these variables supports deeper pattern extraction without compromising model integrity.\n\n### Next Steps\n\nWhile this reduction process enhances efficiency and generalization, the modest R² shows that **structural predictors are still missing**. To evolve from a baseline model to one that captures the complexity of UHI formation:\n\n1. **Reintegrate** `datetime` &  `Location`\n2. **Bring in spatial/land surface data** (e.g., NDVI, impervious surface, land cover types)\n3. **Apply feature importance post-modeling** to further refine and interpret contributions\n\nWith this foundation, the model is better positioned to integrate building, satellite & land-use data that reflect the urban environment’s thermal behavior.","metadata":{}},{"cell_type":"code","source":"# Make a copy to avoid modifying the original dataset\nfinal_train1 = final_train.drop(['Longitude', 'Latitude'], axis=1).copy()\ndata_copy = final_train1.copy()\n\n# Keep coordinates in a metadata DataFrame (exclude from training)  \ntrain_coords = final_train[['Longitude', 'Latitude']]\n\n# Separate features and target\ntarget_col = 'UHI Index'\nX = data_copy.drop(columns=[target_col])\ny = data_copy[target_col]\n\n# Select only numeric features\nX_numeric = X.select_dtypes(include=[float, int])\n\n# Preprocessing\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X_numeric), columns=X_numeric.columns)\nX_scaled.columns = X_scaled.columns.str.replace(r'[\\[\\]<>]', '', regex=True)\nX_numeric.columns = X_numeric.columns.str.replace(r'[\\[\\]<>]', '', regex=True)\n\n# Define features that must always be kept\nfeatures_to_keep = [\n    'Air Temp at Surface degC', \n    'Avg Wind Speed m/s', \n    'Solar Flux W/m^2', \n    'solar_accumulation_6am',\n    'Temp_1hr_MA', \n    'Temp_3hr_MA', \n    'Temp_Change_30min', \n    'Wind Direction degrees', \n    'Wind V', \n    'mins_from_3pm'\n]\n\ndef select_features_for_all_models(X, y, cv=TimeSeriesSplit(5), max_features=20):\n    \"\"\"\n    Optimized feature selection that always retains features_to_keep\n    \"\"\"\n    thresholds = np.linspace(0.7, 0.9, 10)\n    best_score = -np.inf\n    best_features = None\n    \n    # Define models\n    model_set = {\n        'Ridge': Ridge(alpha=1.0),\n        'RandomForest': RandomForestRegressor(\n            n_estimators=50,\n            max_depth=5,\n            min_samples_leaf=2,\n            random_state=42\n        ),\n        'HistGradientBoosting': HistGradientBoostingRegressor(\n            max_iter=50,\n            max_depth=3,\n            learning_rate=0.1,\n            random_state=42\n        )\n    }\n    \n    threshold_results = []\n    \n    for threshold in thresholds:\n        # Calculate correlation matrix\n        corr_matrix = X.corr().abs()\n        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n        \n        # Identify features to drop (excluding protected features)\n        droppable_features = X.columns.difference(features_to_keep + [target_col])\n        to_drop = [col for col in droppable_features if any(upper[col] > threshold)]\n        \n        # Combine protected features with surviving features\n        features = list(set(features_to_keep) | set(X.columns.difference(to_drop + [target_col])))\n        \n        # Skip if too few/many features\n        if len(features) < 3 or len(features) > max_features:\n            continue\n            \n        # Evaluate models\n        model_scores = []\n        for name, model in model_set.items():\n            try:\n                X_data = X_scaled[features] if name == 'Ridge' else X[features]\n                r2_scores = cross_val_score(model, X_data, y, cv=cv, scoring='r2')\n                model_scores.append(np.mean(r2_scores))\n            except Exception as e:\n                model_scores.append(-np.inf)\n        \n        mean_r2 = np.mean(model_scores)\n        threshold_results.append({\n            'threshold': threshold,\n            'n_features': len(features),\n            'mean_r2': mean_r2,\n            'features': features.copy(),\n            'Ridge': model_scores[0],\n            'RandomForest': model_scores[1],\n            'HistGradientBoosting': model_scores[2]\n        })\n        \n        if mean_r2 > best_score:\n            best_score = mean_r2\n            best_features = features\n            best_threshold = threshold\n\n    # Visualization\n    results_df = pd.DataFrame(threshold_results)\n    \n    plt.figure(figsize=(12, 6))\n    plt.scatter(results_df['n_features'], results_df['mean_r2'], \n               c=results_df['threshold'], cmap='viridis', s=100)\n    plt.colorbar(label='Correlation Threshold')\n    plt.xlabel('Number of Features')\n    plt.ylabel('Average R2 Score')\n    plt.title('Feature Selection Performance')\n    \n    best_idx = results_df['mean_r2'].idxmax()\n    plt.scatter(results_df.loc[best_idx, 'n_features'], \n               results_df.loc[best_idx, 'mean_r2'],\n               s=200, edgecolor='red', facecolor='none', linewidth=2)\n    plt.show()\n\n    # Final verification\n    missing_features = set(features_to_keep) - set(best_features)\n    if missing_features:\n        print(\"\\nProtected features automatically added back:\")\n        print(missing_features)\n        best_features = list(set(best_features) | missing_features)\n    \n    print(f\"\\nOptimal feature selection at threshold {best_threshold:.2f}\")\n    print(f\"Selected {len(best_features)} features (max allowed: {max_features})\")\n    print(\"Model Performance:\")\n    print(results_df[['threshold', 'n_features', 'mean_r2', 'Ridge', 'RandomForest', 'HistGradientBoosting']]\n          .sort_values('mean_r2', ascending=False).head(5))\n    \n    return best_features, results_df\n\n# Run feature selection\nprint(\"Running feature selection with protected features...\")\noptimal_features, results_df = select_features_for_all_models(X_scaled, y)\n\n# Create final dataset\nselected_features_df = X_scaled[optimal_features].copy()\nselected_features_df[target_col] = y.values\n\n# Verify protected features are present\nprint(\"\\nProtected features in final dataset:\")\nprint([f for f in features_to_keep if f in selected_features_df.columns])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:33:44.958672Z","iopub.execute_input":"2025-08-01T13:33:44.959024Z","iopub.status.idle":"2025-08-01T13:33:57.625293Z","shell.execute_reply.started":"2025-08-01T13:33:44.958997Z","shell.execute_reply":"2025-08-01T13:33:57.624301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_features_df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:34:16.598467Z","iopub.execute_input":"2025-08-01T13:34:16.598885Z","iopub.status.idle":"2025-08-01T13:34:16.607670Z","shell.execute_reply.started":"2025-08-01T13:34:16.598852Z","shell.execute_reply":"2025-08-01T13:34:16.606495Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Evaluation on Reduced Features (11 Selected via Correlation Thresholding)\n\nI ran 5-fold CV using **Ridge**, **Random Forest**, and **HistGradientBoosting** to test whether the reduced feature set holds meaningful signal.\n\n### Key Metrics & Interpretation\n\n**R² ≈ 0.19**\n* Models explain \\~19% of UHI variance — low, but decent for a baseline.\n* *<0.1* would suggest near-random performance; *0.3+* would indicate moderate predictive power.\n\n**MAE ≈ 0.012**\n* Low MAE + low R² = the models predict average UHI well but miss extremes.\n* This may be “good enough” for general trend prediction, but not for targeting hotspots.\n\n**Stability (Std\\_R² ≈ 0.02, Std\\_MAE ≈ 0.0001)**\n* Low standard deviations across folds imply consistent performance with no signs of overfitting.\n\n### Model-Specific Notes\n* **Ridge (R²: 0.16)**\n  * Underperforms (as expected) due to linearity limits. Still, useful as a stable baseline.\n\n* **Random Forest (R²: 0.18)**\n  * Performs better, but slightly below HistGB.\n\n* **HistGradientBoosting (R²: 0.19)**\n  * Best performer — likely benefits from smooth handling of continuous features.\n\n\n### All of this shows that the models aren’t overfit — they’re **underfed**. \n\n### Next Steps\n* Engineer higher-impact features (e.g., built-up ratio, NDVI, temperature deltas)","metadata":{}},{"cell_type":"code","source":"# Define X_full (added dependency)\nX_full = X_scaled  # or X_numeric if preferred\n\n# MODEL TRAINING & R2 OPTIMIZATION\ndef evaluate_model(model, X, y, model_name, cv=KFold(5, shuffle=True, random_state=42)):\n    \"\"\"Evaluate model with R2 as primary metric, MAE as secondary\"\"\"\n    r2_scores = cross_val_score(model, X, y, cv=cv, scoring='r2')\n    mae_scores = -cross_val_score(model, X, y, cv=cv, scoring='neg_mean_absolute_error')\n    \n    return {\n        'Model': model_name,\n        'Mean_R2': np.mean(r2_scores),\n        'Std_R2': np.std(r2_scores),\n        'Mean_MAE': np.mean(mae_scores),\n        'Std_MAE': np.std(mae_scores)\n    }\n\n# Updated model configurations\nmodels = {\n    'Ridge': Ridge(alpha=1.0),  # Replaced LinearRegression\n    'RandomForest': RandomForestRegressor(\n        n_estimators=200,\n        max_depth=3,\n        min_samples_leaf=2,\n        random_state=42\n    ),\n    'HistGradientBoosting': HistGradientBoostingRegressor(\n        max_iter=150,\n        max_depth=3,\n        learning_rate=0.1,\n        random_state=42\n    )\n}\n\n# Evaluation results (now uses X_full)\nresults = []\nfor name, model in models.items():\n    try:\n        # Use scaled data for Ridge, unscaled for others\n        X_data = X_scaled[optimal_features] if name == 'Ridge' else X_full[optimal_features]\n        results.append(evaluate_model(model, X_data, y, name))\n    except Exception as e:\n        print(f\"Error evaluating {name}: {str(e)}\")\n        continue\n\nif results:\n    results_df = pd.DataFrame(results).sort_values('Mean_R2', ascending=False)\n    print(\"\\nModel Performance Comparison:\")\n    print(results_df.to_string(index=False))\n    \n    # Additional warning system\n    ridge_r2 = results_df[results_df['Model'] == 'Ridge']['Mean_R2'].values[0]\n    best_r2 = results_df.iloc[0]['Mean_R2']\n    if best_r2 - ridge_r2 > 0.3:\n        print(\"\\nWARNING: Large performance gap between Ridge and best model!\")\n        print(\"Potential overfitting - consider:\")\n        print(\"- Using Ridge as your final model\")\n        print(\"- Reducing tree model complexity (max_depth)\")\n        print(\"- Checking feature engineering\")\nelse:\n    print(\"All models failed to evaluate\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:34:23.957108Z","iopub.execute_input":"2025-08-01T13:34:23.957444Z","iopub.status.idle":"2025-08-01T13:34:32.882503Z","shell.execute_reply.started":"2025-08-01T13:34:23.957420Z","shell.execute_reply":"2025-08-01T13:34:32.881472Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance and Model Performance Insights\n### What the Top Features Tell Us\n\nThe most influential features across all models reflect consistent UHI-related signals rooted in **temporal heat accumulation** and **solar exposure**:\n\n1. **Temp\\_3hr\\_MA (0.93)**\n   * The strongest predictor by far—captures cumulative thermal effects\n   * Confirms UHI’s gradual intensification over hours, not minutes\n\n2. **solar\\_accumulation\\_6am (0.51)**\n   * Measures early-day solar energy input\n   * Highlights the **preconditioning effect**: how much heat builds up before peak UHI hours\n\n3. **mins\\_from\\_3pm (0.25)**\n   * Captures **diurnal timing**, indicating how far from the typical peak UHI hour (3 PM) a reading occurs\n   * Reinforces that **time-of-day** is crucial to UHI prediction\n\n4. **Temp\\_1hr\\_MA (0.20)**\n   * Adds a shorter-term signal to complement the 3-hour average\n   * Useful for detecting more **transient shifts** in surface temperature\n\n5. **Air Temp at Surface degC (0.16)**\n   * Provides the **absolute baseline temperature**, but is less influential than derived temporal features\n\n**Key Takeaway**:\nModels are leaning heavily on **smoothed temperature trends** and **early-day solar buildup**, followed by diurnal timing. This matches known UHI behavior, where **thermal inertia** and **timing of energy input** matter more than momentary conditions.\n\n### Final Model Performance Overview\n\n| Model                    | R²     | Notes                                         |\n| ------------------------ | ------ | --------------------------------------------- |\n| **HistGradientBoosting** | 0.1939 | Slightly best—confirms solid generalization   |\n| **Random Forest**        | 0.1936 | Nearly identical—robust across tree ensembles |\n| **Ridge Regression**     | 0.1573 | Weaker, but consistent                        |\n\n**Critical Findings**:\n\n1. **No Overfitting Detected**\n   * Tree-based models show **near-perfect alignment** between CV and final training scores\n   * Suggests good generalization and stable learning from limited features\n\n2. **Ridge Regression Interpretation**\n   * Coefficients reveal a struggle to fit UHI’s nonlinear behavior:\n     * `Temp_3hr_MA: -0.0116`, `Temp_1hr_MA: -0.0066`, `mins_from_3pm: +0.0059`\n   * Implies that **linear models can’t capture time-conditional effects**, where a feature’s impact changes throughout the day\n   * Explains why Ridge underperforms relative to tree-based models\n\n3. **Feature-Limited Ceiling**\n   * All models plateau around **R² = 0.19**\n   * This is not a model failure—it reflects **missing structural and spatial inputs**","metadata":{}},{"cell_type":"code","source":"# FEATURE ANALYSIS & FINAL MODELING\ndef plot_importance_comparison(models_dict, features):\n    \"\"\"Compare feature importance across all models\"\"\"\n    \n    importance_df = pd.DataFrame({'Feature': features})\n    \n    for name, model in models_dict.items():\n        # The models are already fitted in the final_models dictionary,\n        # so we don't need to fit them again here.\n        \n        if hasattr(model, 'feature_importances_'):  # For tree-based models\n            importance = model.feature_importances_\n        elif hasattr(model, 'coef_'):  # For linear models\n            importance = np.abs(model.coef_)\n        else:  # For HistGradientBoosting and similar\n            # Calculate permutation importance\n            result = permutation_importance(\n                model, \n                X_full[features], \n                y,\n                n_repeats=10,\n                random_state=42\n            )\n            importance = result.importances_mean\n            \n        importance_df[f'{name}_Importance'] = importance / importance.max()  # Normalized\n    \n    # Plotting\n    importance_df = importance_df.set_index('Feature')\n    importance_df.plot(kind='barh', figsize=(12, 8), width=0.8)\n    plt.title('Normalized Feature Importance Comparison')\n    plt.xlabel('Normalized Importance Score')\n    plt.ylabel('Features')\n    plt.tight_layout()\n    plt.show()\n    \n    return importance_df\n\n# Fit final models with all data (updated models with the fix)\nfinal_models = {\n    'Ridge': Ridge(alpha=1.0).fit(X_scaled[optimal_features], y),  # Scaled data for Ridge\n    'RandomForest': RandomForestRegressor(\n        max_depth=5,  # Constrained to prevent overfitting\n        random_state=42  # This ensures reproducibility for RandomForest\n    ).fit(X_full[optimal_features], y),  # Unscaled data\n    'HistGradientBoosting': HistGradientBoostingRegressor(\n        max_iter=100,\n        max_depth=3,  # Constrained\n        random_state=42  # ensures reproducibility for HistGradientBoosting\n    ).fit(X_full[optimal_features], y)  # Unscaled data\n}\n\n# Importance analysis\nimportance_df = plot_importance_comparison(final_models, optimal_features)\nprint(\"\\nTop 5 Most Important Features Across All Models:\")\nprint(importance_df.mean(axis=1).sort_values(ascending=False).head(5))\n\n# Final model evaluation\nprint(\"\\nFinal Training R2 Scores:\")\nfor name, model in final_models.items():\n    X_data = X_scaled[optimal_features] if name == 'Ridge' else X_full[optimal_features]\n    y_pred = model.predict(X_data)\n    r2 = r2_score(y, y_pred)\n    print(f\"{name}: {r2:.4f}\")\n    \n    # Additional diagnostic for Ridge model\n    if name == 'Ridge':\n        print(\"  Ridge coefficients:\")\n        for feat, coef in sorted(zip(optimal_features, model.coef_), \n                                 key=lambda x: abs(x[1]), reverse=True)[:5]:\n            print(f\"  {feat}: {coef:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:34:55.532902Z","iopub.execute_input":"2025-08-01T13:34:55.533304Z","iopub.status.idle":"2025-08-01T13:34:59.421916Z","shell.execute_reply.started":"2025-08-01T13:34:55.533273Z","shell.execute_reply":"2025-08-01T13:34:59.420036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_and_position_columns(main_df, source_df, columns_to_add, position='front'):\n    \"\"\"\n    Safely adds columns from source_df to main_df and positions them as specified\n    \n    Args:\n        main_df (pd.DataFrame): DataFrame to add columns to\n        source_df (pd.DataFrame): DataFrame containing columns to add\n        columns_to_add (list): Columns to transfer\n        position (str): Where to position new columns ('front' or 'end')\n    \n    Returns:\n        pd.DataFrame: DataFrame with added and positioned columns\n    \"\"\"\n    # Validate inputs\n    if not all(col in source_df.columns for col in columns_to_add):\n        raise ValueError(\"Some columns not found in source DataFrame\")\n    \n    if len(main_df) != len(source_df):\n        raise ValueError(\"DataFrames must have same length\")\n    \n    # Create a copy to avoid SettingWithCopyWarning\n    result_df = main_df.copy()\n    \n    # Add original index for safe merging\n    result_df['_temp_index'] = source_df.index\n    \n    # Merge with source columns\n    result_df = result_df.merge(\n        source_df[columns_to_add],\n        left_on='_temp_index',\n        right_index=True,\n        how='left'\n    ).drop('_temp_index', axis=1)\n    \n    # Verify merge succeeded\n    verify_merge(result_df, source_df, columns_to_add)\n    \n    # Reorder columns\n    return position_columns(result_df, columns_to_add, position)\n\ndef verify_merge(result_df, source_df, columns_to_add):\n    \"\"\"Verify that the merge was successful\"\"\"\n    # Check for null values\n    missing_data = result_df[columns_to_add].isnull().sum()\n    print(\"\\nMerge Verification:\")\n    for col in columns_to_add:\n        print(f\"Missing {col} values: {missing_data[col]}\")\n        if missing_data[col] > 0:\n            raise ValueError(f\"{missing_data[col]} missing values found in {col}\")\n    \n    # Spot check random rows\n    sample_indices = random.sample(range(len(result_df)), min(5, len(result_df)))\n    print(\"\\nSample Verification:\")\n    for idx in sample_indices:\n        for col in columns_to_add:\n            original_val = source_df.iloc[idx][col]\n            new_val = result_df.iloc[idx][col]\n            print(f\"Index {idx}: {col} - Original: {original_val} | New: {new_val}\")\n            if original_val != new_val:\n                raise ValueError(f\"Mismatch found in {col} at index {idx}\")\n\ndef position_columns(df, columns_to_position, position):\n    \"\"\"Reposition columns in DataFrame\"\"\"\n    all_cols = df.columns.tolist()\n    positioned_cols = [col for col in all_cols if col not in columns_to_position]\n    \n    if position == 'front':\n        new_order = columns_to_position + positioned_cols\n    elif position == 'end':\n        new_order = positioned_cols + columns_to_position\n    else:\n        raise ValueError(\"Position must be 'front' or 'end'\")\n    \n    return df[new_order]\n\n# Usage example:\nselected_features_df = add_and_position_columns(\n    main_df=selected_features_df,\n    source_df=final_train,\n    columns_to_add=['datetime', 'Location'],\n    position='front'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:35:55.231287Z","iopub.execute_input":"2025-08-01T13:35:55.231697Z","iopub.status.idle":"2025-08-01T13:35:55.258333Z","shell.execute_reply.started":"2025-08-01T13:35:55.231669Z","shell.execute_reply":"2025-08-01T13:35:55.257317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_features_df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:36:05.398341Z","iopub.execute_input":"2025-08-01T13:36:05.398717Z","iopub.status.idle":"2025-08-01T13:36:05.410581Z","shell.execute_reply.started":"2025-08-01T13:36:05.398690Z","shell.execute_reply":"2025-08-01T13:36:05.409703Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Number of unique entries in each column\")\ndisplay(selected_features_df.nunique())  # See if any numeric columns have suspiciously few values\n\nprint(\"Current Train Data Shape\")\ndisplay(selected_features_df.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:36:08.892202Z","iopub.execute_input":"2025-08-01T13:36:08.892551Z","iopub.status.idle":"2025-08-01T13:36:08.908016Z","shell.execute_reply.started":"2025-08-01T13:36:08.892524Z","shell.execute_reply":"2025-08-01T13:36:08.907023Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"selected_features_df.describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Applying Preprocessing to the Test Dataset","metadata":{}},{"cell_type":"code","source":"# Apply weather feature engineering to test data\nfinal_test = preprocess_weather_features(test_df)\nfinal_test1 = final_test.drop(['Longitude', 'Latitude'], axis=1).copy()\n\n# Keep coordinates in a metadata DataFrame (exclude from training)  \ntest_coords = final_test[['Longitude', 'Latitude']]\n\n# Define the final columns in the exact required order\nFINAL_COLUMNS = ['datetime', 'Location', 'solar_accumulation_6am', 'Wind V',\n       'Temp_Change_30min', 'Temp_3hr_MA', 'Wind Direction degrees',\n       'Solar Flux W/m^2', 'Avg Wind Speed m/s', 'mins_from_3pm',\n       'Temp_1hr_MA', 'Air Temp at Surface degC', 'UHI Index']\n\n# 1. Scale the test data FIRST (before renaming columns)\n# Make sure we only scale the numeric features (excluding datetime, Location, UHI Index)\nnumeric_cols = [col for col in final_test1.columns \n                if col not in ['datetime', 'Location', 'UHI Index'] \n                and pd.api.types.is_numeric_dtype(final_test1[col])]\n\n# Apply the fitted scaler to these columns\nfinal_test1[numeric_cols] = scaler.transform(final_test1[numeric_cols])\n\n# 2. Remove special characters from column names\nfinal_test1.columns = final_test1.columns.str.replace(r'[\\[\\]<>]', '', regex=True)\n\n# 3. Select and reorder only the final 14 columns\n# First verify all required columns exist\nmissing_cols = set(FINAL_COLUMNS) - set(final_test1.columns)\nif missing_cols:\n    raise ValueError(f\"Missing required columns after renaming: {missing_cols}\")\n\n# Keep only the final columns in the correct order\nfinal_test1 = final_test1[FINAL_COLUMNS]\n\n# Verify the final output\nprint(\"Final test dataset shape:\", final_test1.shape)\nprint(\"\\nFirst 2 rows:\")\nprint(final_test1.head(2))\nprint(\"\\nColumn order:\")\nprint(final_test1.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:36:22.113479Z","iopub.execute_input":"2025-08-01T13:36:22.113849Z","iopub.status.idle":"2025-08-01T13:36:22.148682Z","shell.execute_reply.started":"2025-08-01T13:36:22.113812Z","shell.execute_reply":"2025-08-01T13:36:22.147658Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Working with Building footprint","metadata":{}},{"cell_type":"code","source":"'''# Path to the KML file\nkml_file = \"/kaggle/input/ey-challenge/Building_Footprint.kml\"\n\n# Path to save the shapefile\nshapefile_path = \"/kaggle/working/Building_Footprint.shp\"\n\n# Run the ogr2ogr command to convert KML to shapefile\nsubprocess.run(['ogr2ogr', '-f', 'ESRI Shapefile', shapefile_path, kml_file])\n\nprint(f\"Shapefile saved at {shapefile_path}\")\n'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shapefile_path = \"/kaggle/input/ey-challenge/Building_Footprint/Building_Footprint.shp\"\n\n# Read the shapefile into a GeoDataFrame\nbuilding_gdf = gpd.read_file(shapefile_path)\n\n# View the first few rows of the GeoDataFrame\ndisplay(building_gdf.head(3))\ndisplay(building_gdf.info())\n\nprint(\"\\n Building Data Shape:\")\ndisplay(building_gdf.shape)\n\n# Get CRS for the gdf shapefile\nprint(\"CRS: {}\".format(building_gdf.crs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:37:01.359459Z","iopub.execute_input":"2025-08-01T13:37:01.359841Z","iopub.status.idle":"2025-08-01T13:37:02.934901Z","shell.execute_reply.started":"2025-08-01T13:37:01.359809Z","shell.execute_reply":"2025-08-01T13:37:02.933903Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Mapping Urban Heat Over Building Footprints in NYC\n\nAs part of the ongoing exploration of Urban Heat Islands (UHI), I wanted to take a closer look at how heat patterns interact with the built environment—specifically across parts of New York City. The goal here is to visually connect heat intensity with where buildings are actually located on the ground.\n\n### UHI Values Plotted Over Building Footprints (Bronx & Manhattan)\n\nThis map gives a spatial snapshot of how UHI values align with NYC’s urban layout. Here’s how to interpret what we’re seeing:\n\n* **Base Map**: The underlying street map helps us locate ourselves within Manhattan and the Bronx. Streets, parks, and water bodies are visible to give context.\n\n* **Building Footprints**: Represented in green, these show the outlines of physical structures. They highlight where buildings are clustered and help us compare urban density across neighborhoods.\n\n* **UHI Points (Main Layer of Interest)**: The colored dots overlaid on the map reflect different levels of UHI intensity:\n\n  * **Red & Orange** → Higher heat intensity (our \"hot spots\").\n  * **Blue tones** → Lower heat intensity, often cooler zones with less dense construction or more vegetation.\n\n* **Labeled Boroughs**: Manhattan and the Bronx are clearly marked to help orient the viewer and allow us to compare trends between them.\n\n### What Are We Seeing? (And Why It Matters)\n\nLooking at the visual, a few things stand out:\n\n* **The Bronx has noticeably more high-UHI points** than Manhattan. This lines up with existing research that’s flagged the Bronx—especially the South Bronx—as particularly heat-vulnerable. Factors like dense low-income housing, fewer trees, and limited access to cooling resources likely play a role.\n\n* **Central Park’s cooling effect is clear**. That large patch of blue in the center of Manhattan isn’t random—it matches up with Central Park. Parks and green spaces tend to reduce heat by replacing concrete with vegetation, which cools the air through evapotranspiration. This is a textbook example of nature offsetting the heat from the surrounding built-up areas.\n\n\nThis visualisation strengthens the link between what we see in the data and what’s happening on the ground. It also sharpens our focus as we move toward modeling UHI more precisely—not just as a surface temperature issue, but as a spatial and environmental justice one. \n\n**For further context:**\n\n* [Climate Justice in NYC – Mayor’s Office](https://climate.cityofnewyork.us/ht/ejnyc-report/the-state-of-environmental-justice-in-nyc/exposure-to-climate-change/)\n* [Heat Mapping Study Finds Higher Temps in Lower-Income Neighborhoods – Columbia Climate School](https://justicenetwork.climate.columbia.edu/news/nyc-heat-mapping-study-finds-higher-temps-lower-income-neighborhoods)\n* [NYC’s Heat Island Effect – FOX 5 New York](https://www.fox5ny.com/news/nyc-experiencing-heat-island-effect-amid-soaring-temperatures)\n* [Fixing NYC’s Heat Wave Inequity – NYU Stern EDG](https://www.nyusternedg.org/editorial/nyc-heat-waves-are-inequitable-heres-how-to-fix-them)\n","metadata":{}},{"cell_type":"code","source":"# Convert final_df to GeoDataFrame\nuhi_gdf = gpd.GeoDataFrame(\n    final_df.copy(),\n    geometry=gpd.points_from_xy(final_df['Longitude'], final_df['Latitude']),\n    crs='EPSG:4326'\n)\n\n# Reproject for plotting\nuhi_gdf = uhi_gdf.to_crs(epsg=3857)\nbuilding_gdf = building_gdf.to_crs(epsg=3857)\n\n# Separate UHI by location\nbronx = uhi_gdf[uhi_gdf[\"Location\"] == \"Bronx\"]\nmanhattan = uhi_gdf[uhi_gdf[\"Location\"] == \"Manhattan\"]\n\n# Plot everything\nfig, ax = plt.subplots(figsize=(14, 14))\n\n# Plot building footprints: green edges\nbuilding_gdf.plot(ax=ax, facecolor=\"none\", edgecolor=\"green\", linewidth=0.5, alpha=0.6)\n\n# Plot UHI points: color shows intensity\nuhi_plot = uhi_gdf.plot(\n    ax=ax,\n    column=\"UHI Index\",\n    cmap=\"coolwarm\",\n    markersize=40,\n    legend=True,\n    alpha=0.8\n)\n\n# Add basemap\nctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik, alpha=0.9)\n\n# Label Bronx and Manhattan\nbronx_centroid = bronx.unary_union.centroid\nmanhattan_centroid = manhattan.unary_union.centroid\n\nax.text(bronx_centroid.x, bronx_centroid.y, \"Bronx\", fontsize=16, fontweight='bold', color='navy')\nax.text(manhattan_centroid.x, manhattan_centroid.y, \"Manhattan\", fontsize=16, fontweight='bold', color='darkred')\n\n# Add Custom Legend\nuhi_patch = mpatches.Patch(color=\"orange\", label=\"High UHI Index\")\ncool_patch = mpatches.Patch(color=\"blue\", label=\"Low UHI Index\")\nbuilding_patch = mpatches.Patch(edgecolor=\"green\", facecolor=\"none\", label=\"Building Footprints\")\n\nplt.legend(handles=[uhi_patch, cool_patch, building_patch], loc=\"lower left\", fontsize=10)\n\n# Final formatting\nax.set_title(\"Urban Heat Island Points over Building Footprints (Bronx & Manhattan)\", fontsize=18)\nax.set_axis_off()\nplt.tight_layout()\nplt.show()\n\n# Reproject back to EPSG:4326\nuhi_gdf = uhi_gdf.to_crs(epsg=4326)\nbuilding_gdf = building_gdf.to_crs(epsg=4326)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:39:35.612147Z","iopub.execute_input":"2025-08-01T13:39:35.612579Z","iopub.status.idle":"2025-08-01T13:39:42.257214Z","shell.execute_reply.started":"2025-08-01T13:39:35.612547Z","shell.execute_reply":"2025-08-01T13:39:42.256106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Select relevant columns\nbuilding_df = building_gdf[['id', 'fid', 'layer','geometry']].copy()\n\n# Display the first few rows\nprint(building_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:39:58.480868Z","iopub.execute_input":"2025-08-01T13:39:58.481269Z","iopub.status.idle":"2025-08-01T13:39:58.492522Z","shell.execute_reply.started":"2025-08-01T13:39:58.481235Z","shell.execute_reply":"2025-08-01T13:39:58.491485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Clean the 'layer' column to extract borough names (e.g., \"clip_Bronx\" → \"Bronx\")\nbuilding_df[\"Location\"] = building_df[\"layer\"].str.replace(\"clip_\", \"\")\n\n# Drop unused columns (optional)\nbuilding_df = building_df[[\"id\", \"Location\", \"geometry\"]]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:40:04.561304Z","iopub.execute_input":"2025-08-01T13:40:04.561698Z","iopub.status.idle":"2025-08-01T13:40:04.573870Z","shell.execute_reply.started":"2025-08-01T13:40:04.561666Z","shell.execute_reply":"2025-08-01T13:40:04.572763Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"CRS: {}\".format(building_gdf.crs))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T13:40:08.860966Z","iopub.execute_input":"2025-08-01T13:40:08.861298Z","iopub.status.idle":"2025-08-01T13:40:08.866298Z","shell.execute_reply.started":"2025-08-01T13:40:08.861275Z","shell.execute_reply":"2025-08-01T13:40:08.865204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"building_df[\"Location\"].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:09:56.144355Z","iopub.execute_input":"2025-08-01T14:09:56.144907Z","iopub.status.idle":"2025-08-01T14:09:56.155265Z","shell.execute_reply.started":"2025-08-01T14:09:56.144857Z","shell.execute_reply":"2025-08-01T14:09:56.154002Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Basic EDA – Understanding the Urban Landscape of Bronx and Manhattan\n\nBefore modeling Urban Heat Island (UHI) effects, I ran a foundational analysis to understand the layout and density of buildings in the Bronx and Manhattan — since the built environment plays a direct role in how heat is stored and dispersed.\n\n### Building Count and Area\n\n* **Bronx:** 5,926 buildings covering \\~76.7 million sq ft\n* **Manhattan:** 3,510 buildings covering \\~126.3 million sq ft\n\nThough the Bronx has more buildings, Manhattan’s structures are significantly larger on average — about **36,000 sq ft** compared to **13,000 sq ft** in the Bronx. This aligns with the expected difference in urban form: the Bronx has more low-rise residential structures, while Manhattan is shaped by larger commercial and high-rise buildings.\n\n### What This Means for UHI\n\n* The **Bronx’s heat** may stem from the **dense clustering** of smaller buildings, which limits airflow and increases surface heat retention.\n* In **Manhattan**, while buildings are larger, vertical structures and parks (like Central Park) contribute to **shading and ventilation**, creating localized cooling effects.\n\nThis contrast helps explain patterns seen in earlier UHI visualizations and sets a foundation for why borough-level structural differences matter in heat modeling.","metadata":{}},{"cell_type":"code","source":"# Preprocess Data \n# Convert to projected CRS (EPSG:2263 for feet)\nbuilding_df = building_df.to_crs(epsg=2263)\n\n# Calculate areas\nbuilding_df[\"area\"] = building_df.geometry.area  # Area in square feet\n\n# Borough Statistics \nborough_stats = building_df.groupby(\"Location\").agg(\n    num_buildings=(\"id\", \"count\"),\n    total_area=(\"area\", \"sum\"),\n    avg_area=(\"area\", \"mean\")\n).reset_index()\n\n# Format numbers to avoid scientific notation\nborough_stats[\"total_area\"] = borough_stats[\"total_area\"].round(2)\nborough_stats[\"avg_area\"] = borough_stats[\"avg_area\"].round(2)\n\n# Print Summary Stats \nprint(\"\\nBuilding Count per Borough\")\nprint(building_df[\"Location\"].value_counts())\n\nprint(\"\\nTotal Building Area (sq ft)\")\nprint(building_df.groupby(\"Location\")[\"area\"].sum().round(2))\n\nprint(\"\\nBorough Statistics\")\ndisplay(borough_stats)\n\n# Side-by-Side Plots \nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n\n# Bronx Plot\nbronx = building_df[building_df[\"Location\"] == \"Bronx\"]\nbronx.plot(ax=ax1, alpha=0.5, color='blue')\nax1.set_title(\"Bronx Building Footprints\", fontsize=14)\nax1.set_axis_off()\n\n# Manhattan Plot\nmanhattan = building_df[building_df[\"Location\"] == \"Manhattan\"]\nmanhattan.plot(ax=ax2, alpha=0.5, color='red')\nax2.set_title(\"Manhattan Building Footprints\", fontsize=14)\nax2.set_axis_off()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:13:38.295716Z","iopub.execute_input":"2025-08-01T14:13:38.296165Z","iopub.status.idle":"2025-08-01T14:13:41.095572Z","shell.execute_reply.started":"2025-08-01T14:13:38.296136Z","shell.execute_reply":"2025-08-01T14:13:41.094283Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Deeper Dive – Advanced EDA of the Built Environment and Urban Heat Dynamics\n\nTo complement basic building stats, this analysis explores how geometry, density, and spatial design shape UHI behavior in Bronx and Manhattan. Instead of just surface area, we now look at how **form and spacing** influence thermal dynamics.\n\n### Urban Form and Spatial Layout\n\n| Metric                  | Bronx    | Manhattan |\n| ----------------------- | -------- | --------- |\n| Building Count          | 5,926    | 3,510     |\n| Total Area (sq ft)      | 76.7M    | 126.3M    |\n| Avg Building Area       | \\~13,000 | \\~36,000  |\n| Median NN Distance (ft) | 123.25      |  204.88       |\n\n* The **Bronx** has a more **compact and horizontally spread** urban layout, while **Manhattan** is denser vertically, with wider gaps between buildings.\n* **Shorter distances** between Bronx buildings can lead to **less airflow** and **more trapped heat**, forming a more continuous heat layer.\n* **Manhattan’s spacing**, while looser, combines with complex structures that create **localized hotspots** and **thermal contrasts**.\n\n### Building Shape and Ventilation Potential\n\n| Metric             | Bronx | Manhattan |\n| ------------------ | ----- | --------- |\n| Mean Compactness   | 0.57  | 0.49      |\n| % Highly Irregular | 2.45%    | 6.70%        |\n\n* The Bronx’s more **regular shapes** support smoother airflow, while Manhattan’s irregular forms may **disrupt ventilation**, increasing microclimate variability.\n* These differences are important: **irregular, complex buildings** can block wind and **retain heat longer**, especially in tightly packed areas.\n\n### Large Structures and Thermal Imbalance\n\nOutliers matter: nearly **800 buildings exceed 3× the 75th percentile in size**, including highly irregular structures.\n\n* One Bronx building spans over **768,000 sq ft** with extremely low compactness — behaving as a **thermal anchor** that stores and slowly releases heat.\n\nThese buildings don’t just affect themselves — they alter conditions in surrounding blocks, intensifying UHI effects in nearby residential areas.\n\n### Urban Inequality and Heat Exposure\n\n* **Manhattan shows higher inequality** in building size, leading to abrupt transitions from small buildings to skyscrapers. These shifts can cause **uneven heat distribution**, creating “cool gaps” by day that become **radiant zones** at night.\n* In contrast, the **Bronx’s more uniform structure** leads to a **flatter but more consistent UHI surface** — fewer extremes, but less relief.\n\n### Summary: Contrasting Heat Behaviors\n\n| Factor           | Bronx                          | Manhattan                              |\n| ---------------- | ------------------------------ | -------------------------------------- |\n| Urban Form       | Compact, low-rise              | Tall, irregular, mixed-density         |\n| Heat Behavior    | Continuous surface heat        | Patchy, localized hotspots             |\n| Primary Risk     | Persistent heat accumulation   | Thermal imbalance + night radiation    |\n| Mitigation Focus | Broad cooling (trees, roofing) | Targeted solutions (ventilation paths) |","metadata":{}},{"cell_type":"code","source":"# 1. Calculate Building Compactness\n# Compactness is a shape metric: 1 for a perfect circle, lower for irregular shapes.\n# Formula: 4 * pi * Area / Perimeter^2\n# We use 3.14159 for pi for consistency with common approximations in such calculations.\nbuilding_df['compactness'] = 4 * 3.14159 * building_df['area'] / (building_df.geometry.length**2)\n\n# 2. Calculate Average Nearest Neighbor Distance \n# Measures building density by finding the average distance to the 5 closest building centroids.\ncoords = np.column_stack([building_df.geometry.centroid.x, building_df.geometry.centroid.y])\nnbrs = NearestNeighbors(n_neighbors=5, algorithm='kd_tree').fit(coords)\ndistances, _ = nbrs.kneighbors(coords)\nbuilding_df['avg_neighbor_dist_ft'] = distances.mean(axis=1)\n\n# --- Insights and Feature Aggregation ---\nprint(\"Advanced EDA: Built Environment and Urban Heat Dynamics\\n\")\n\n# Calculate metrics for \"Urban Form and Spatial Layout\" and \"Building Shape and Ventilation Potential\" tables.\n# This combines calculations needed for multiple rows and sections of your write-up.\nborough_features = building_df.groupby('Location').agg(\n    # Building Count (for table)\n    building_count=('Location', 'size'),\n    # Total Area (sq ft) (for table)\n    total_area_sqft=('area', 'sum'),\n    # Average Building Area (for table)\n    avg_building_area=('area', 'mean'),\n    # Median Nearest Neighbor Distance (for table)\n    median_nn_distance_ft=('avg_neighbor_dist_ft', 'median'),\n    # Mean Compactness (for table)\n    mean_compactness=('compactness', 'mean'),\n    # Percentage of Highly Irregular Buildings (for table)\n    # Defined as compactness < 0.2, as per your Block 3 original logic.\n    pct_highly_irregular=('compactness', lambda x: (x < 0.2).mean() * 100) # Express as percentage\n)\n\nprint(\"Urban Form and Spatial Layout & Building Shape and Ventilation Potential Metrics\")\n# Format for readability, matching your table's approximate values.\n# Note: 'total_area_sqft' will be large, might need manual formatting for 'M' in markdown table.\nprint(borough_features.round(2).to_string())\nprint(\"\\n\" + \"=\"*80 + \"\\n\") # Separator for clarity\n\n\n# Analyze Outliers: Large Structures and Thermal Imbalance\nq75_area = building_df['area'].quantile(0.75)\n# Filter buildings exceeding 3x the 75th percentile area.\noutliers_large = building_df[building_df['area'] > q75_area * 3]\nprint(f\"Large Structures and Thermal Imbalance Outliers\")\nprint(f\"Nearly {len(outliers_large):,} buildings exceed 3x the 75th percentile in size ({q75_area * 3:,.0f} sq ft).\\n\")\n\n# Find the largest Bronx building with low compactness\n# Assuming \"extremely low compactness\" implies looking for a compactness value below a certain threshold, e.g., 0.2\nbronx_thermal_anchor = outliers_large[\n    (outliers_large['Location'] == 'Bronx') &\n    (outliers_large['compactness'] < 0.2) # Use the \"highly irregular\" threshold for \"extremely low\"\n].sort_values('area', ascending=False).head(1)\n\nif not bronx_thermal_anchor.empty:\n    print(f\"Specific thermal anchor example (Bronx, largest irregular outlier):\")\n    print(f\"- Area: {bronx_thermal_anchor['area'].iloc[0]:,.0f} sq ft\")\n    print(f\"- Compactness: {bronx_thermal_anchor['compactness'].iloc[0]:.3f}\\n\")\nelse:\n    print(\"Could not find a specific large Bronx building with extremely low compactness among outliers based on criteria.\\n\")\n\nprint(\"Largest buildings among these outliers (top 5 by area):\")\nprint(outliers_large[['Location', 'area', 'compactness']].sort_values('area', ascending=False).head(5).to_string())\nprint(\"\\n\" + \"=\"*80 + \"\\n\") # Separator for clarity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:13:44.680319Z","iopub.execute_input":"2025-08-01T14:13:44.680721Z","iopub.status.idle":"2025-08-01T14:13:44.775776Z","shell.execute_reply.started":"2025-08-01T14:13:44.680686Z","shell.execute_reply":"2025-08-01T14:13:44.774751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\n Building Data Shape:\")\nprint(building_df.shape)\n\nprint(\"\\n Building Data CRS:\")\nprint(\"CRS: {}\".format(building_df.crs))\n\nprint(\"\\n Building Data Unique Entries:\")\nprint(building_df.nunique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:14:07.796963Z","iopub.execute_input":"2025-08-01T14:14:07.797471Z","iopub.status.idle":"2025-08-01T14:14:07.856084Z","shell.execute_reply.started":"2025-08-01T14:14:07.797430Z","shell.execute_reply":"2025-08-01T14:14:07.854712Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"'''# Reproject back to EPSG:4326\nbuilding_df = building_df.to_crs(epsg=4326)'''","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Engineering Spatial Features from Building Footprints\n\nTo better predict Urban Heat Island (UHI) intensity, I transformed the NYC building footprint dataset into spatial features that reflect the urban form around each UHI point. Since there wasn’t a one-to-one mapping between UHI coordinates and individual buildings, I adopted a spatial summarization approach.\n\n### Why This Was Needed\n* **Mismatch in data sizes**:\n  The building dataset (9,436 entries) didn’t match the combined train/test UHI entries (11,229).\n* **No direct join**:\n  UHI points only had latitude and longitude, and I couldn't use these directly as features.\n* **Goal**:\n  Capture *contextual building characteristics* around each UHI point without violating the constraints.\n\n### The Strategy\n1. **Buffer Zones**:\n   Around each UHI coordinate, I created circular zones with radii of:\n\n   * 50m\n   * 100m\n   * 150m\n   * 200m\n\n2. **Within Each Buffer**, I computed:\n   * Count of buildings\n   * Summary stats (mean, median, std, sum) for:\n\n     * `area`\n     * `compactness`\n     * `avg_neighbor_dist_ft`\n   * **Built-up ratio**:\n     Total building area ÷ buffer area (a proxy for urban density)\n\n3. **Why Buffers Work**:\n   * UHI is influenced by the *surrounding built form*, not individual structures.\n   * Buffers allow us to approximate urban context without using GPS directly.\n\n### Result\n* These aggregated spatial features enriched the dataset with localized urban morphology.\n* The model now has access to *urban density, compactness, and spacing patterns* across multiple scales—improving its ability to learn how the environment influences heat distribution.","metadata":{}},{"cell_type":"code","source":"# Reset the index of train_coords to match the order of selected_features_df\ntrain_coords = train_coords.reset_index(drop=True)\n\n# Now check\nassert selected_features_df.index.equals(train_coords.index)\n\n# Now check\nassert final_test1.index.equals(test_coords.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:15:40.394281Z","iopub.execute_input":"2025-08-01T14:15:40.394722Z","iopub.status.idle":"2025-08-01T14:15:40.400240Z","shell.execute_reply.started":"2025-08-01T14:15:40.394688Z","shell.execute_reply":"2025-08-01T14:15:40.399097Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_coords.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:15:43.803144Z","iopub.execute_input":"2025-08-01T14:15:43.803500Z","iopub.status.idle":"2025-08-01T14:15:43.812802Z","shell.execute_reply.started":"2025-08-01T14:15:43.803469Z","shell.execute_reply":"2025-08-01T14:15:43.811602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_spatial_features(uhi_coords_df, buildings_gdf, buffer_sizes=[50, 100, 150, 200]):\n    \"\"\"\n    Engineered spatial features from building footprints within buffers around UHI points.\n    \n    Args:\n        uhi_coords_df: DataFrame with 'latitude' and 'longitude' columns\n        buildings_gdf: GeoDataFrame of building footprints with metrics\n        buffer_sizes: List of buffer radii in meters\n        \n    Returns:\n        DataFrame with engineered features (no geometry/coordinates)\n    \"\"\"\n    # Convert to GeoDataFrame in Web Mercator (meters)\n    gdf = gpd.GeoDataFrame(\n        uhi_coords_df,\n        geometry=gpd.points_from_xy(uhi_coords_df.Longitude, uhi_coords_df.Latitude),\n        crs=\"EPSG:4326\"\n    ).to_crs(\"EPSG:3857\")  # Convert to meter-based CRS\n    \n    # Buffer buildings to avoid repeated spatial joins\n    buildings_buffered = buildings_gdf.to_crs(\"EPSG:3857\").copy()\n    \n    results = []\n    for radius in buffer_sizes:\n        # Create buffers\n        buffered = gdf.copy()\n        buffered['geometry'] = buffered.geometry.buffer(radius)\n        buffered['buffer_area'] = np.pi * (radius ** 2)  # Area of circular buffer\n        \n        # Spatial join with buildings\n        joined = gpd.sjoin(buffered, buildings_buffered, how='left', predicate='intersects')\n        \n        # Aggregate features\n        agg_features = joined.groupby(level=0).agg({\n            'area': ['count', 'mean', 'median', 'std', 'sum'],\n            'compactness': ['mean', 'median', 'std'],\n            'avg_neighbor_dist_ft': ['mean', 'median', 'std'],\n        })\n        \n        # Flatten multi-index columns\n        agg_features.columns = [f\"{radius}m_{stat}_{var}\" \n                              for var, stat in agg_features.columns]\n        \n        # Align buffer areas with aggregated features\n        buffer_areas = buffered['buffer_area'].loc[agg_features.index]\n        agg_features[f\"{radius}m_builtup_ratio\"] = (\n            agg_features[f\"{radius}m_sum_area\"] / buffer_areas\n        )\n        \n        results.append(agg_features)\n    \n    # Combine all buffer sizes\n    return pd.concat(results, axis=1).fillna(0)\n\n# Example usage:\nif __name__ == \"__main__\":\n    # 1. Generate spatial features\n    train_spatial = create_spatial_features(train_coords, building_df)\n    test_spatial = create_spatial_features(test_coords, building_df)\n    \n    # 2. Merge with existing data (excluding coordinates)\n    train_enhanced = pd.concat([selected_features_df, train_spatial], axis=1)\n    test_enhanced = pd.concat([final_test1, test_spatial], axis=1)\n\n    # Move 'UHI Index' to last column (for both train and test)\n    for i, df in enumerate([train_enhanced, test_enhanced]):\n        if \"UHI Index\" in df.columns:\n            # Store UHI values\n            uhi_values = df[\"UHI Index\"].copy()\n            # Drop the column\n            df.drop(columns=[\"UHI Index\"], inplace=True)\n            # Re-add as last column\n            df[\"UHI Index\"] = uhi_values        \n\n# Verify\nprint(\"Last 5 train columns:\", train_enhanced.columns.tolist()[-5:])\nprint(\"Last 5 test columns:\", test_enhanced.columns.tolist()[-5:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:15:47.397062Z","iopub.execute_input":"2025-08-01T14:15:47.397389Z","iopub.status.idle":"2025-08-01T14:15:50.254262Z","shell.execute_reply.started":"2025-08-01T14:15:47.397364Z","shell.execute_reply":"2025-08-01T14:15:50.253265Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def check_missingness(df, label=\"Dataset\"):\n    missing_percent = (df.isnull().sum() / len(df)) * 100\n    print(f\"\\nMissing values (%): {label}\")\n    return missing_percent[missing_percent > 0].sort_values(ascending=False)\n\ncheck_missingness(train_enhanced, \"Enhanced Train\") \ncheck_missingness(test_enhanced, \"Enhanced Test\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:16:05.582215Z","iopub.execute_input":"2025-08-01T14:16:05.582592Z","iopub.status.idle":"2025-08-01T14:16:05.597834Z","shell.execute_reply.started":"2025-08-01T14:16:05.582561Z","shell.execute_reply":"2025-08-01T14:16:05.596854Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Quick look at first rows\nprint(\"Train Dataset\")\ndisplay(train_enhanced.head(3))  # Reduced to 3 rows for brevity\n\nprint(\"Test Dataset\")\ndisplay(test_enhanced.head(3))  # Reduced to 3 rows for brevity\n\n# Shape (row x column count)\nprint(\"Train Dataset Shape:\", train_enhanced.shape)\nprint(\"Test Dataset Shape:\", test_enhanced.shape)\n\ndef show_uniqueness_extremes(df, name=\"Dataset\"):\n    nunique = df.nunique()\n    print(f\"\\n{name} - Top 5 columns with highest uniqueness:\")\n    display(nunique.sort_values(ascending=False).head(5))\n    print(f\"\\n{name} - Top 5 columns with lowest uniqueness:\")\n    display(nunique.sort_values().head(5))\n\nshow_uniqueness_extremes(train_enhanced, \"Train Enhanced\")\nshow_uniqueness_extremes(test_enhanced, \"Test Enhanced\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:16:09.433488Z","iopub.execute_input":"2025-08-01T14:16:09.433874Z","iopub.status.idle":"2025-08-01T14:16:09.537702Z","shell.execute_reply.started":"2025-08-01T14:16:09.433842Z","shell.execute_reply":"2025-08-01T14:16:09.536777Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Buffer Visualization: Purpose and Reasoning\n\nThis visualization illustrates how buffer radius affects the spatial context captured around UHI measurement points. The goal is to understand how different buffer sizes influence the building features aggregated for modeling Urban Heat Island intensity.\n\nThe buffers are generated around each UHI location. Within each buffer, building-level attributes—such as total footprint area, number of structures, and shape descriptors—are computed. These spatial features are then used as input to the regression model predicting UHI index.\n\n### Why This Matters\n\nThe choice of buffer size is critical. It defines the spatial boundary within which environmental features are considered relevant. A buffer that is too small may miss surrounding context, while one that is too large may dilute local effects with unrelated data.\n\nThis visualization allows for:\n* A direct comparison of spatial coverage across buffer sizes.\n* Visual verification that buffers are correctly aligned with UHI locations.\n* An intuitive understanding of how much building data is included at each radius.\n\n### Technical Notes\n* Buffers are created around UHI points, not buildings.\n* The radius can be adjusted dynamically from 50m to 200m.\n* A subset of points and buildings is sampled to ensure performance and clarity.\n* Buffers are styled to visually reflect scale differences across settings.\n\n### Use Case in Modeling\nBy observing how features vary with buffer size, I can empirically test which radius results in the most informative spatial features—measured by R² on validation data. This visualization directly supports that decision process and helps justify buffer choices in the final modeling pipeline.","metadata":{}},{"cell_type":"code","source":"def create_interactive_buffer_map(uhi_points, buildings, min_buffer=50, max_buffer=200):\n    \"\"\"\n    Fully interactive map where buffer circles:\n    - Resize dynamically with slider\n    - Change color (blue -> red) based on size\n    - Update in real-time\n    \"\"\"\n    # Convert to WGS84 if not already\n    # Ensure uhi_points and buildings are GeoDataFrames with a 'geometry' column\n    # and have a CRS set before attempting to_crs.\n    if uhi_points.crs != \"EPSG:4326\":\n        uhi_points = uhi_points.to_crs(\"EPSG:4326\")\n    if buildings.crs != \"EPSG:4326\":\n        buildings = buildings.to_crs(\"EPSG:4326\")\n\n    # Create base map\n    # Ensure uhi_points is not empty before accessing iloc[0]\n    if uhi_points.empty:\n        print(\"UHI points GeoDataFrame is empty. Cannot create map center.\")\n        return None\n    center = [uhi_points.geometry.iloc[0].y, uhi_points.geometry.iloc[0].x]\n    m = folium.Map(\n        location=center,\n        zoom_start=15,\n        tiles='CartoDB positron'\n    )\n\n    # Sample buildings for performance\n    # Ensure buildings is not empty before sampling\n    if not buildings.empty:\n        buildings_sample = buildings.sample(min(1000, len(buildings))) if len(buildings) > 1000 else buildings\n        folium.GeoJson(\n            buildings_sample,\n            name='Buildings',\n            style_function=lambda x: {'color': 'orange', 'weight': 0.5, 'fillOpacity': 0.3}\n        ).add_to(m)\n\n    # Color scale for buffers\n    colormap = cm.LinearColormap(\n        ['blue', 'yellow', 'red'],\n        vmin=min_buffer, vmax=max_buffer,\n        caption='Buffer Radius (m)'\n    )\n    colormap.add_to(m)\n\n    # Create a container for buffers ONCE and add it to the map\n    # This FeatureGroup will be cleared and refilled, not removed and re-added.\n    buffer_container = folium.FeatureGroup(name=\"Buffers\")\n    m.add_child(buffer_container)\n\n    def update_buffers(buffer_size=100):\n        \"\"\"Dynamically update buffers\"\"\"\n        # Clear all existing circles from the buffer_container\n        # by resetting its internal _children dictionary.\n        # This is the standard way to clear a FeatureGroup's contents in Folium.\n        buffer_container._children = OrderedDict()\n\n        # Sample UHI points for clarity (ensure not empty)\n        if not uhi_points.empty:\n            sample_points = uhi_points.sample(min(50, len(uhi_points)))\n\n            # Add new buffers to the existing buffer_container\n            for idx, row in sample_points.iterrows():\n                folium.Circle(\n                    location=[row.geometry.y, row.geometry.x],\n                    radius=buffer_size,\n                    color=colormap(buffer_size),\n                    fill=True,\n                    fill_opacity=0.2,\n                    popup=f\"Buffer: {buffer_size}m\",\n                ).add_to(buffer_container) # Add to the pre-existing FeatureGroup\n\n        # Print current buffer size for feedback (appears below the map)\n        print(f\"Current buffer size: {buffer_size}m\")\n        # IMPORTANT: Return the map object so interact can display the updated map\n        return m\n\n    # Create slider\n    slider = IntSlider(\n        value=100,\n        min=min_buffer,\n        max=max_buffer,\n        step=10,\n        description='Buffer Size:',\n        continuous_update=True  # Update while dragging\n    )\n\n    # Add LayerControl to the map (this is done once)\n    folium.LayerControl().add_to(m)\n\n    # Interactive widget. This function call itself handles the initial display\n    # and subsequent updates of the interactive map and the slider.\n    # The `update_buffers` function will be called with the slider's value,\n    # and its return value (the updated map 'm') will be displayed.\n    interact(update_buffers, buffer_size=slider)\n\n    return None # Or simply remove this line entirely for implicit None return","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:16:17.100241Z","iopub.execute_input":"2025-08-01T14:16:17.100574Z","iopub.status.idle":"2025-08-01T14:16:17.111192Z","shell.execute_reply.started":"2025-08-01T14:16:17.100550Z","shell.execute_reply":"2025-08-01T14:16:17.109981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Usage (completely separate from main pipeline):\nif __name__ == \"__main__\":\n    # Prepare data (using your existing variables)\n    uhi_points = gpd.GeoDataFrame(\n        train_coords,\n        geometry=gpd.points_from_xy(train_coords.Longitude, train_coords.Latitude),\n        crs=\"EPSG:4326\"\n    )\n    \n    # Generate interactive map\n    buffer_map = create_interactive_buffer_map(uhi_points, building_df)\n    \n    # For Jupyter:\n    if buffer_map: # Only display if map was successfully created\n        display(buffer_map)\n    # For saving: buffer_map.save(\"buffer_explorer.html\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:16:24.484107Z","iopub.execute_input":"2025-08-01T14:16:24.484447Z","iopub.status.idle":"2025-08-01T14:16:25.360788Z","shell.execute_reply.started":"2025-08-01T14:16:24.484422Z","shell.execute_reply":"2025-08-01T14:16:25.359548Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Urban Buffer Feature Summary\n\nAfter engineering spatial features using building footprint data within buffers of 50m to 200m around each UHI observation point, we generated a set of descriptive metrics to characterize the built environment at various scales. These features aim to capture urban morphology, density, and spatial arrangement, all of which can influence UHI intensity.\n\n### 1. **Feature Groups**\nEach buffer radius (50m, 100m, 150m, 200m) includes the following types of metrics:\n\n* `count_area`: Number of buildings within the buffer.\n* `mean_area`, `median_area`, `std_area`, `sum_area`: Distribution of building sizes (in square meters).\n* `mean_compactness`, `median_compactness`, `std_compactness`: Shape complexity of buildings; values closer to 1 indicate more compact shapes.\n* `mean_avg_neighbor_dist_ft`, etc.: Average distance between buildings, capturing spatial dispersion.\n* `builtup_ratio`: Ratio of built area to total buffer area — a proxy for local urban density.\n\n### 2. **General Observations**\n\n* **Scale progression:** As buffer radius increases, both the number of buildings and cumulative building area (`sum_area`) increase. This shows spatial coverage expanding as expected.\n  \n* **Compactness:**\n  * Mean compactness values range from \\~0.38 (50m) to \\~0.48 (200m), suggesting moderate shape regularity.\n  * Standard deviation of compactness remains low (\\~0.1–0.17), indicating limited variability in shape complexity across buildings.\n\n* **Neighbor distances:**\n  * Mean distance between neighboring buildings increases slightly with buffer size (from \\~194ft to \\~217ft), suggesting more spread out buildings in wider buffers.\n  * The large maximum of \\~900ft likely corresponds to outliers or isolated buildings.\n\n* **Built-up Ratio:**\n  * Decreases with increasing buffer size (from \\~13% at 50m to \\~3.5% at 200m), indicating that urban density thins out at broader scales.\n\n### 3. **Distribution Insights**\n\n* **Highly skewed distributions** are observed in many features:\n  * Maximum values (e.g., `sum_area`, `std_area`) are disproportionately high, indicating the presence of dense urban pockets or outlier zones.\n  * Several features have **0 values at the 25th percentile**, especially at smaller buffers. This means a significant number of UHI points fall in low or non-built-up areas at close range — a valuable insight for urban heat prediction.\n\n### 4. **Modeling Implications**\n\n* **Feature selection** or **transformation** (e.g., log-scaling or binning) may help normalize highly skewed distributions.\n* **Built-up ratio** and **neighbor distance metrics** show meaningful variation across scales, which could be important for learning scale-dependent UHI patterns.\n* The consistency in compactness and area-related stats across scales suggests strong underlying spatial patterns, which can be leveraged by tree-based models like Random Forest or Gradient Boosting.\n\n### 5. **Next Steps**\n\n* Consider **correlation analysis** across buffer levels to detect redundant features.\n* Use model-based feature importance to identify which buffer scale captures the most predictive spatial context.\n* Explore whether **multi-scale feature aggregation** (e.g., min/max across scales) offers more generalizable insights than individual buffer sizes alone.","metadata":{}},{"cell_type":"code","source":"# Get the column slice from index 12 to 60 (Python indexing is exclusive on the stop index)\nengineered_cols = train_enhanced.columns[12:61]\n\n# Describe only the engineered buffer features\ntrain_enhanced[engineered_cols].describe()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:16:41.283187Z","iopub.execute_input":"2025-08-01T14:16:41.283539Z","iopub.status.idle":"2025-08-01T14:16:41.407262Z","shell.execute_reply.started":"2025-08-01T14:16:41.283509Z","shell.execute_reply":"2025-08-01T14:16:41.406131Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Urban Structure Features and Their Relationship with UHI\n\nThis analysis looks at how urban structural characteristics—like building area, compactness, and spacing—measured across different buffer zones (50m to 200m) relate to the Urban Heat Island (UHI) Index. The goal is to understand how features of the built environment might influence local heat intensity.\n\n### Key Takeaways\n\n#### 1. Consistency Within Buffers\n\n* **Area Metrics** (mean, median, sum) are tightly correlated within each buffer. For example, in the 50m zone, `mean_area` and `median_area` are almost identical (corr ≈ 0.98). These features all reflect the size of buildings nearby.\n* **Built-up Ratio** is nearly perfectly correlated with `sum_area` (≈ 1.00) in each buffer, as both essentially capture total built footprint.\n* **Compactness & Neighbor Distance** metrics show high internal agreement too—mean and median values for each are tightly aligned across zones.\n\n#### 2. Consistency Across Buffers\n\n* Structural features are also correlated *across* buffer sizes. E.g., `50m_mean_area` has strong positive relationships with `100m_mean_area`, `150m_mean_area`, etc.\n* This tells us that areas with dense or compact buildings in a small zone (50m) tend to remain so in broader areas (100m–200m).\n\n#### 3. Relationship with UHI Index\n\n* **Low to Moderate Correlation:** The UHI Index shows only weak to modest relationships with individual structural features. For example:\n\n  * `200m_median_area`: 0.28\n  * `150m_builtup_ratio`: 0.26\n* **Larger Buffers Matter Slightly More:** Metrics from larger buffers (150m–200m) show slightly stronger correlation with UHI than immediate surroundings (50m–100m), suggesting broader urban context may better explain heat patterns.\n* **No Single Driver:** No one feature clearly dominates; UHI seems influenced by a *combination* of spatial and environmental factors.\n\n### Technical Considerations\n\n* **Multicollinearity Alert:** Features like `mean_area`, `sum_area`, and `builtup_ratio` carry redundant information. For modeling, it’s worth reducing this overlap using techniques like PCA or regularized regression.\n* **Correlation ≠ Causation:** These are *absolute* correlation values. They show strength, not direction. Also, they capture only linear relationships; more complex, non-linear effects could be at play.\n* **Exploratory, Not Predictive:** These stats help us understand patterns, but don’t yet explain or predict UHI directly.\n\n### Final Thoughts\n\nUrban form clearly has a structured internal logic—buildings that are large tend to be close together, and built-up areas cluster across space. However, their impact on UHI appears subtle and spread out. This calls for deeper modeling that blends structural, material, and environmental data to fully capture what drives urban heat.","metadata":{}},{"cell_type":"code","source":"# Compute absolute correlation matrix\nencorr_matrix = train_enhanced[engineered_cols].corr().abs()\n\ndisplay(encorr_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:16:56.143187Z","iopub.execute_input":"2025-08-01T14:16:56.143516Z","iopub.status.idle":"2025-08-01T14:16:56.263194Z","shell.execute_reply.started":"2025-08-01T14:16:56.143493Z","shell.execute_reply":"2025-08-01T14:16:56.261943Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Sanity Check on Buffer-Based Spatial Features\n\nBefore training models using spatial features derived from buffer zones, I performed a statistical sanity check to assess their overall quality and behavior across different buffer sizes. This step helps ensure that the features introduced are not just numerous, but also meaningful and diverse enough to contribute to predictive performance.\n\n### What Was Measured?\n\nFor each buffer size (50m, 100m, 150m, 200m), I evaluated:\n\n* **Number of Features:** Total number of spatial metrics generated.\n* **Mean Variance:** Average variance across features. High variance typically suggests more information spread, which is useful for modeling.\n* **Zero Variance Features:** Count of features that do not vary across observations (i.e., are constant). These are redundant for training.\n* **Mean Absolute Correlation with Target:** Average correlation (absolute value) between each feature and the UHI Index. While low correlation doesn't always mean low utility (due to non-linear relationships), it's still a helpful proxy.\n\n### Key Observations\n\n* All buffer sizes contributed **12 spatial features** each, ensuring a uniform structure.\n* **No zero-variance features** were found, which indicates that all features have some level of variation across UHI locations.\n* **Mean variance increases** significantly with buffer size, peaking at 200m:\n\n  * 50m: \\~1.26B\n  * 100m: \\~2.08B\n  * 150m: \\~3.79B\n  * 200m: \\~5.23B\n    This trend suggests that larger buffers capture more diverse or aggregated spatial patterns, possibly due to more buildings or features falling within the area.\n* **Mean correlation with the UHI Index** is relatively low across the board (0.01–0.02), which is expected given the noisy and multifactorial nature of urban heat patterns.\n\n### Interpretation & Next Steps\n\n* The increase in variance with buffer size is promising—it suggests larger buffers introduce more differentiation between UHI points.\n* Since no zero-variance features were present, all features can be considered valid inputs for modeling.\n* Although the average correlation with the target is low, tree-based models like Random Forest and HistGradientBoosting can still uncover non-linear patterns, so these features are worth testing in modeling.\n\n**Next step:** Run model training experiments across buffer sizes to empirically determine which buffer range contributes most to predictive performance.","metadata":{}},{"cell_type":"code","source":"# Sanity Check Groupings\ntarget = train_enhanced['UHI Index']\nbuffer_sizes = ['50m', '100m', '150m', '200m']\nsummary_stats = []\n\nfor buf in buffer_sizes:\n    # Select features for this buffer size\n    buf_cols = [col for col in df.columns if col.startswith(f\"{buf}_\")]\n    X_buf = df[buf_cols]\n    \n    # Variance per feature\n    variances = X_buf.var()\n    \n    # Correlation with target\n    corrs = X_buf.corrwith(target)\n    \n    summary_stats.append({\n        'buffer_size': buf,\n        'num_features': len(buf_cols),\n        'mean_variance': variances.mean(),\n        'num_zero_var': (variances == 0).sum(),\n        'mean_abs_corr_with_target': corrs.abs().mean()\n    })\n\nbuffer_summary_df = pd.DataFrame(summary_stats)\nprint(buffer_summary_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:17:21.776966Z","iopub.execute_input":"2025-08-01T14:17:21.777356Z","iopub.status.idle":"2025-08-01T14:17:21.847580Z","shell.execute_reply.started":"2025-08-01T14:17:21.777329Z","shell.execute_reply":"2025-08-01T14:17:21.846525Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Filtering Buffer Features with Domain-Aware Selection\n\nThis section outlines how we reduced buffer feature complexity to improve model performance and interpretability for UHI prediction.\n\n### What Are Buffer Features?\n\nBuffer features describe the urban form around each point at various spatial scales (50m, 100m, 150m, 200m). These include:\n\n* **Area metrics** (`mean`, `median`, `sum`, `std` of area)\n* **Compactness** (how clustered buildings are)\n* **Neighbor distance** (average spacing between structures)\n* **Built-up ratio** (share of land covered by buildings)\n* **Count** (number of structures in the buffer)\n\nThese features reflect how the built environment may contribute to local heat patterns.\n\n### Step 1: Feature Filtering Using Pearson Correlation\n\nWe used **Pearson correlation** to measure how each buffer feature relates to the UHI Index. This helped highlight the most impactful and least redundant features.\n\n#### Key Patterns:\n\n**Strong Positive Correlations (↑ Feature → ↑ UHI):**\n\n* `count_area` — e.g., `200m_count_area` (0.22), `150m_count_area` (0.19): More buildings correlate with higher UHI.\n* `std_compactness` — Greater variability in building shape/density increases UHI.\n* `std_avg_neighbor_dist_ft` — Irregular spacing is linked to higher UHI.\n\n**Strong Negative Correlations (↑ Feature → ↓ UHI):**\n\n* `mean_area` and `median_area` — e.g., `200m_median_area` (-0.28), `150m_median_area` (-0.26): Larger and more widely spaced buildings tend to reduce UHI.\n* `avg_neighbor_dist_ft` — Greater average spacing between buildings lowers UHI.\n* `builtup_ratio` and `sum_area` — Weakly negative, possibly due to overlap with more strongly correlated features.\n\n### Buffer Size Preference\n\nThe **150m and 200m buffer sizes** consistently showed stronger correlations—both positive and negative—across key feature types. This suggests that **broader spatial contexts are more informative** for understanding UHI than immediate surroundings.\n\n* Top positive features: `200m_count_area` (0.22), `150m_count_area` (0.19)\n* Top negative features: `200m_median_area` (-0.28), `150m_mean_area` (-0.23)\n\nIn contrast, **50m and 100m** buffers showed weaker correlations for many metrics, indicating that they capture less useful variation for UHI prediction in this context.\n\n**Summary:**\n* Features tied to **building count, size, and spacing** are the most informative.\n* **Larger buffer zones (150–200m)** outperform smaller ones in correlation strength.\n* This step ensures we retain meaningful, non-redundant features grounded in urban form theory.","metadata":{}},{"cell_type":"code","source":"# Define your engineered columns\nprint(f\"Original engineered buffer columns: {engineered_cols}\")\n\n# Step 1: Calculate Pearson Correlation with UHI Index\nprint(\"\\nStep 1: Pearson Correlation with UHI Index\")\n\n# Separate features and target\nX_buffer = train_enhanced[engineered_cols]\ny_uhi = train_enhanced['UHI Index']\n\ncorrelations = X_buffer.corrwith(y_uhi).sort_values(ascending=False)\nprint(\"Correlation of each buffer feature with UHI_Index:\")\nprint(correlations)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:17:29.063155Z","iopub.execute_input":"2025-08-01T14:17:29.063483Z","iopub.status.idle":"2025-08-01T14:17:29.096411Z","shell.execute_reply.started":"2025-08-01T14:17:29.063456Z","shell.execute_reply":"2025-08-01T14:17:29.095344Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 2: Domain-Aware Buffer Feature Selection\n\nThis step refines the feature set by selecting the most relevant **buffer size** for **each urban characteristic**, ensuring diversity and reducing redundancy.\n\n#### Why This Step Matters\n\n* Avoids picking multiple similar features across buffer sizes.\n* Keeps one **best** feature per type (e.g., `mean_area`, `std_compactness`).\n* Improves interpretability by tying each feature to its most meaningful spatial scale.\n\n#### How It Works\n\nFor each feature type, we select the version (e.g., 50m, 100m, etc.) with the **highest absolute Pearson correlation** to the UHI Index. This gives one optimal feature per category.\n\n#### Key Results – Buffer Sizes Favored\n\n**Selection varies by feature type**, showing that UHI drivers operate at different spatial scales:\n\n* **50m** — Favored for local metrics like:\n  * `mean_area`, `median_area`, `std_compactness`, `builtup_ratio`\n  * Suggests immediate surroundings play a strong role in some UHI effects\n\n* **100m** — Selected for:\n  * `count_area`, `std_avg_neighbor_dist_ft`, and others\n  * Captures slightly broader variation beyond the immediate footprint\n\n* **150m** — Favored for:\n  * Aggregate spacing and area metrics like `mean_avg_neighbor_dist_ft`, `median_area`\n  * Indicates that neighborhood-scale morphology matters for UHI\n\n* **200m** — Frequently selected for:\n  * Broader urban form traits like `sum_area`, `median_compactness`, `builtup_ratio`\n  * Reflects influence from wider urban context (e.g., parks, block layouts)\n\n#### Why This Matters for UHI Modeling\n\nUHI effects stem from a mix of **micro-scale** (e.g., building spacing) and **macro-scale** (e.g., overall urban density) features. This method ensures that each selected feature reflects its most meaningful spatial influence.\n\n\n#### Summary\n\n* One best feature per type → less redundancy, more interpretability\n* Favored buffer sizes vary → consistent with how UHI behaves across scales\n* Sets the stage for cleaner modeling and later multicollinearity checks","metadata":{}},{"cell_type":"code","source":"# Step 2: Domain-Aware Selection based on Correlation \nprint(\"\\nStep 2: Domain-Aware Selection based on Correlation\")\n\n# Group features by their base name (e.g., 'building_area', 'vegetation_coverage')\nfeature_types = {}\nfor col in engineered_cols:\n    base_name = '_'.join(col.split('_')[:-1]) # e.g., 'building_area' from 'building_area_50m'\n    if base_name not in feature_types:\n        feature_types[base_name] = []\n    feature_types[base_name].append(col)\n\nselected_features_by_correlation = []\nfor base_name, cols in feature_types.items():\n    type_correlations = correlations.loc[cols].abs() # Use absolute correlation for selection\n    best_col = type_correlations.idxmax()\n    selected_features_by_correlation.append(best_col)\n    print(f\"For '{base_name}', selected '{best_col}' with |correlation|: {type_correlations.max():.4f}\")\n\nprint(\"\\nFeatures initially selected based on highest individual correlation per type:\")\nprint(selected_features_by_correlation)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:17:41.674266Z","iopub.execute_input":"2025-08-01T14:17:41.674602Z","iopub.status.idle":"2025-08-01T14:17:41.715661Z","shell.execute_reply.started":"2025-08-01T14:17:41.674575Z","shell.execute_reply":"2025-08-01T14:17:41.714365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 3: Multicollinearity Check with VIF\n\nThis step checks for **redundancy** among the selected buffer features using **Variance Inflation Factor (VIF)**—a measure of how much a feature overlaps with others. While tree models can tolerate some overlap, reducing multicollinearity improves:\n\n* **Interpretability:** Easier to understand what features truly matter\n* **Efficiency:** Less noise, faster training\n* **Model stability:** More distinct signal from each feature\n\n#### How It Works\n\nWe calculate the VIF for each feature.\n\n* **VIF = 1:** No multicollinearity\n* **1 < VIF < 5:** Moderately correlated.\n* **VIF >= 5 or 10:** Highly correlated, suggesting a problem with multicollinearity. Infinite VIF (inf) indicates perfect multicollinearity.\n\n#### Key Findings\n\n##### Perfect or Extremely High VIFs (∞ or >100)\n\n* **`sum_area` and `builtup_ratio`** (across all buffer sizes) show perfect overlap. They often measure the same thing—total vs. proportion of built-up space.\n* **Neighbor Distance Averages** (`mean`, `median`): e.g., `50m_mean_avg_neighbor_dist_ft` ≈ 311 VIF. These strongly overlap with density and area metrics.\n\n##### High VIFs (10–100)\n\n* **Average Area Features** (`mean_area`, `median_area`): VIFs 30–44\n* **Building Count (`count_area`)**: VIFs 9–28\n  → All are interrelated measures of density and scale.\n\n##### Low VIFs (<5)\n\n* **Heterogeneity Metrics** (`_std_area`, `_std_compactness`, `_std_avg_neighbor_dist_ft`) generally show low VIFs.\n  → These describe **variation**, not just quantity, making them more distinct and valuable for modeling.\n\n#### Buffer Size Insights\n\nWhile no buffer size is explicitly \"favored\" at this stage, features based on **standard deviation** consistently show **lower multicollinearity**, regardless of buffer radius. This suggests **variability-based metrics** offer more unique information compared to simpler average or sum-based features.\n\n\n#### **Summary:**\n\n* High redundancy exists—even among \"best\" features from Step 2.\n* **Area, count, and density features** are especially prone to overlap.\n* **Heterogeneity metrics** (e.g., std-based) are more independent.\n* Next step: prune overlapping features to create a cleaner, more effective feature set.","metadata":{}},{"cell_type":"code","source":"# Step 3: Multicollinearity Check using VIF\nprint(\"\\nStep 3: Multicollinearity Check using VIF\")\n\n# Add a constant to the DataFrame for VIF calculation (needed for intercept in regression)\n# VIF is calculated for features intended to be in a linear model.\n# Even if your final model is XGBoost, high VIF in predictors can still indicate redundancy.\nX_vif = train_enhanced[selected_features_by_correlation]\nX_vif = add_constant(X_vif) # Adds a constant column for VIF calculation\n\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_vif.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_vif.values, i) for i in range(X_vif.shape[1])]\n\n# Remove the 'const' row from VIF results for clarity\nvif_data = vif_data[vif_data['feature'] != 'const']\nvif_data = vif_data.sort_values(by='VIF', ascending=False)\n\nprint(\"\\nVIF for initially selected features:\")\nprint(vif_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:17:54.727479Z","iopub.execute_input":"2025-08-01T14:17:54.727877Z","iopub.status.idle":"2025-08-01T14:17:55.228005Z","shell.execute_reply.started":"2025-08-01T14:17:54.727845Z","shell.execute_reply":"2025-08-01T14:17:55.226929Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 4: Final Feature Refinement Using VIF + Domain Knowledge\n\nThis step combines **statistical filtering** (VIF < 5) with **domain expertise** to finalize the buffer feature set. The goal is to keep features that are:\n\n* Predictive of UHI\n* Non-redundant\n* Interpretable\n\n#### How It Works\n\nAn iterative process:\n\n1. Calculate VIF for all features\n2. Remove the one with the highest VIF (if VIF > 5)\n3. Recalculate and repeat until all VIFs are below the threshold\n\n#### What Got Removed (and Why)\n\n##### First to Go: Perfectly Redundant (VIF = ∞)\n\n* **`_sum_area` and `_builtup_ratio`**: Measure the same thing (impervious surface), so only one is needed.\n* **`_median_avg_neighbor_dist_ft`** and **`_median_area`**: Highly similar to their corresponding mean-based versions.\n\n##### Next to Go: Still Redundant (VIF > 5)\n\n* **`_mean_area`** and **`_count_area`** at larger buffers: Overlap with density and spacing metrics.\n* **`_mean_avg_neighbor_dist_ft`**: Highly correlated with area and count features.\n* **Remaining `_builtup_ratio`** values: Continued redundancy even after early removals.\n\nThese removals reflect the same theme: **dense, average-based features tend to overlap**, especially across buffer sizes.\n\n\n#### Final Feature Set (19 Buffer Features)\n\n| Buffer   | Selected Features                                                                                         |\n| -------- | --------------------------------------------------------------------------------------------------------- |\n| **50m**  | `count_area`, `median_area`, `std_compactness`, `mean_avg_neighbor_dist_ft`, `std_avg_neighbor_dist_ft` |\n| **100m** | `median_area`, `std_compactness`, `sum_area`, `mean_avg_neighbor_dist_ft`, `std_avg_neighbor_dist_ft`     |\n| **150m** | `std_compactness`, `median_avg_neighbor_dist_ft`, `std_avg_neighbor_dist_ft`                                            |\n| **200m** | `count_area`, `median_area`, `std_area`, `sum_area`, `mean_avg_neighbor_dist_ft`, `std_avg_neighbor_dist_ft`                                       |\n\n\n#### Key Insights\n\n* **Variability Wins**: Standard deviation metrics (`std_compactness`, `std_avg_neighbor_dist_ft`, `std_area`) dominate, showing they capture more **unique**, non-redundant information.\n* **Spacing Matters**: `mean_avg_neighbor_dist_ft` appears across multiple buffers, highlighting the role of **building arrangement and spacing** in UHI dynamics.\n* **Simple Counts & Averages Still Useful—But Only at Select Scales**: A few `count_area`, `mean_area`, and `builtup_ratio` features survived, mainly where they provided less overlap.\n\n#### Final Takeaway\nAfter removing redundancy, the most informative features are those that describe the **variability and spatial structure** of the urban environment—not just how much is built, but **how it's arranged**. This aligns with emerging UHI research: **urban heterogeneity and spacing drive local heat patterns more than simple density alone.**","metadata":{}},{"cell_type":"code","source":"def vif_feature_selection(\n    df,\n    feature_columns=None,\n    vif_threshold=5.0,\n    exclude_columns=None,\n    verbose=True\n):\n    \"\"\"\n    Dynamically removes high-VIF features from a DataFrame.\n    This version includes robustness checks for perfect multicollinearity\n    and LinAlgError.\n    \"\"\"\n    # Determine which features to evaluate\n    if feature_columns is None:\n        if exclude_columns is None:\n            exclude_columns = ['datetime', 'Location', 'UHI Index']\n        feature_columns = [col for col in df.select_dtypes(include='number').columns if col not in exclude_columns]\n    \n    final_features = list(feature_columns)\n    iteration = 1\n\n    while True:\n        X = df[final_features].copy()\n        \n        # Remove any columns with zero variance, as they cause VIF issues\n        X = X.loc[:, (X != X.iloc[0]).any()]\n        final_features = list(X.columns)\n\n        if X.shape[1] < 2:\n            if verbose:\n                print(\"\\nLess than two features remaining. VIF selection complete.\")\n            break\n        \n        X_const = add_constant(X, has_constant='add')\n        vif_df = pd.DataFrame()\n        vif_df['feature'] = X_const.columns\n        \n        # Calculate VIFs with an error check\n        vif_values = []\n        for i in range(X_const.shape[1]):\n            try:\n                vif = variance_inflation_factor(X_const.values, i)\n                vif_values.append(vif)\n            except LinAlgError:\n                vif_values.append(np.inf) # Set to infinity if the calculation fails\n\n        vif_df['VIF'] = vif_values\n        vif_df = vif_df[vif_df['feature'] != 'const'].sort_values(by='VIF', ascending=False).reset_index(drop=True)\n        \n        # Check for highest VIF above threshold, including inf\n        high_vif_row = vif_df[vif_df['VIF'] > vif_threshold].head(1)\n        if high_vif_row.empty:\n            if verbose:\n                print(f\"\\nNo features with VIF > {vif_threshold}. Selection complete.\")\n            break\n            \n        feature_to_remove = high_vif_row.iloc[0]['feature']\n        vif_value = high_vif_row.iloc[0]['VIF']\n        final_features.remove(feature_to_remove)\n\n        if verbose:\n            print(f\"\\n[Iteration {iteration}] Removing '{feature_to_remove}' (VIF = {vif_value:.2f})\")\n            print(f\"Remaining features: {len(final_features)}\")\n            print(vif_df)\n            iteration += 1\n\n    # Final VIF table for selected features\n    if final_features:\n        final_X = add_constant(df[final_features], has_constant='add')\n        final_vif_df = pd.DataFrame()\n        final_vif_df['feature'] = final_X.columns\n        final_vif_df['VIF'] = [variance_inflation_factor(final_X.values, i) for i in range(final_X.shape[1])]\n        final_vif_df = final_vif_df[final_vif_df['feature'] != 'const'].sort_values(by='VIF', ascending=False).reset_index(drop=True)\n    else:\n        final_vif_df = pd.DataFrame(columns=['feature', 'VIF'])\n\n    return final_features, final_vif_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:18:04.925163Z","iopub.execute_input":"2025-08-01T14:18:04.925502Z","iopub.status.idle":"2025-08-01T14:18:04.937529Z","shell.execute_reply.started":"2025-08-01T14:18:04.925477Z","shell.execute_reply":"2025-08-01T14:18:04.936203Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 4: Refine Selection based on VIF and Domain Knowledge\nprint(\"\\nStep 4: Refining Selection based on VIF and Domain Knowledge\")\nselected_features_ = [f for f in selected_features_by_correlation if f != 'UHI Index']\nfinal_features, vif_report = vif_feature_selection(\n    df=train_enhanced,\n    feature_columns=selected_features_,\n    vif_threshold=5.0\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:18:08.023025Z","iopub.execute_input":"2025-08-01T14:18:08.023351Z","iopub.status.idle":"2025-08-01T14:18:12.843434Z","shell.execute_reply.started":"2025-08-01T14:18:08.023327Z","shell.execute_reply":"2025-08-01T14:18:12.842375Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nNumber of features after initial VIF\")\nlen(final_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:19:11.155199Z","iopub.execute_input":"2025-08-01T14:19:11.155554Z","iopub.status.idle":"2025-08-01T14:19:11.162475Z","shell.execute_reply.started":"2025-08-01T14:19:11.155529Z","shell.execute_reply":"2025-08-01T14:19:11.161413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def filter_selected_features(df, selected_buffer_features, target_column='UHI Index', buffer_start_idx=12, buffer_end_idx=60):\n    \"\"\"\n    Filters a DataFrame to include only non-buffer features, selected buffer features, and the target variable.\n\n    Parameters:\n        df (pd.DataFrame): The input DataFrame.\n        selected_buffer_features (list): List of selected buffer feature column names.\n        target_column (str): Name of the target variable column (default is 'UHI Index').\n        buffer_start_idx (int): Index where buffer features start in the original dataset.\n        buffer_end_idx (int): Index where buffer features end (inclusive).\n\n    Returns:\n        pd.DataFrame: Filtered DataFrame with selected columns.\n    \"\"\"\n    all_columns = df.columns.tolist()\n    non_buffer_features = all_columns[:buffer_start_idx]  # Adjust based on actual structure\n    columns_to_keep = non_buffer_features + selected_buffer_features + [target_column]\n\n    df_filtered = df[columns_to_keep].copy()\n\n    print(f\"Original number of features: {df.shape[1]}\")\n    print(f\"Number of features after selection: {df_filtered.shape[1]}\")\n    print(\"New DataFrame columns:\", df_filtered.columns.tolist())\n\n    return df_filtered","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:19:32.524424Z","iopub.execute_input":"2025-08-01T14:19:32.524870Z","iopub.status.idle":"2025-08-01T14:19:32.530929Z","shell.execute_reply.started":"2025-08-01T14:19:32.524835Z","shell.execute_reply":"2025-08-01T14:19:32.529789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the function for train\ntrain_enhanced_fil = filter_selected_features(train_enhanced, final_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:19:41.143566Z","iopub.execute_input":"2025-08-01T14:19:41.143985Z","iopub.status.idle":"2025-08-01T14:19:41.154008Z","shell.execute_reply.started":"2025-08-01T14:19:41.143947Z","shell.execute_reply":"2025-08-01T14:19:41.152979Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the same function for test\ntest_enhanced_fil = filter_selected_features(test_enhanced, final_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:19:46.197015Z","iopub.execute_input":"2025-08-01T14:19:46.197389Z","iopub.status.idle":"2025-08-01T14:19:46.206574Z","shell.execute_reply.started":"2025-08-01T14:19:46.197358Z","shell.execute_reply":"2025-08-01T14:19:46.205507Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Interpreting the Correlation Matrix: Buffer vs. Non-Buffer Features\n\nThis analysis explores how selected **buffer features** (urban structure at different radii) relate to **non-buffer features** (meteorological and temporal data). The goal is to check for redundancy and understand whether both sets offer complementary value for modeling.\n\n### 1. Key Relationships\n\n* **Temperature Metrics (1hr/3hr Moving Averages, Surface Temp):**\n\n  * **Moderately correlated** with buffer features like `builtup_ratio`, `mean_area`, and `count_area`, especially at **100m–150m**.\n  * Indicates that **denser areas tend to be warmer**, as expected.\n  * **Variability metrics** (`std_`) show weaker temperature links, suggesting they capture more nuanced spatial characteristics.\n\n* **Solar Accumulation (6am):**\n\n  * **Negatively correlated** with built-up metrics (e.g., `-0.39` with `150m_builtup_ratio`), suggesting **denser zones receive less early sunlight**—possibly due to shading or obstruction.\n\n* **30-Min Temperature Change:**\n\n  * Weak correlations overall, but **slightly stronger with variability features** like `std_avg_neighbor_dist_ft`, hinting at links between **structural complexity** and short-term temp shifts.\n\n* **Wind and Time Variables:**\n\n  * **Very low or no correlation** with buffer features—urban form seems to have **minimal direct impact** on these at this resolution.\n\n### 2. Patterns Among Buffer Features\n\n* **Count, Mean, and Built-Up Metrics:**\n\n  * Show **clear relationships** with atmospheric features, reinforcing that **urban density and size influence local climate**.\n\n* **Standard Deviation Features:**\n\n  * Generally **uncorrelated with non-buffer features**, indicating they capture **unique spatial variability**.\n\n### 3. Redundancy Check\n\n* No **high correlations (> 0.8)** between buffer and non-buffer features.\n* Moderate overlaps are **expected and meaningful**, not problematic.\n* Confirms VIF results: buffer features provide **distinct structural signals**, not noise.\n\n### Conclusion\n\nThe selected buffer features offer **complementary, non-redundant information** alongside meteorological variables. Their relationships are intuitive—urban form shapes heat and light exposure—yet they still capture unique structural traits. This validates the next step: analyzing how urban features **change across distances**, now on a solid foundation.\n","metadata":{}},{"cell_type":"code","source":"# Keep only numeric columns\nnumeric_df = train_enhanced_fil.select_dtypes(include='number')\n\n# Compute correlation matrix\ncorrfil_matrix = numeric_df.corr()\n\n# Display correlation matrix\ndisplay(corrfil_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:19:53.209258Z","iopub.execute_input":"2025-08-01T14:19:53.209593Z","iopub.status.idle":"2025-08-01T14:19:53.263380Z","shell.execute_reply.started":"2025-08-01T14:19:53.209568Z","shell.execute_reply":"2025-08-01T14:19:53.262361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Capturing Urban Gradients with Delta & Ratio Features\nThis strategy enhances the predictive power of UHI models by moving beyond static buffer features and introducing **engineered features** that quantify how urban characteristics **change across space**. Instead of only using values like `builtup_ratio` at a specific radius (e.g., 50m or 100m), we generate:\n\n* **Delta features** — the **absolute change** in a variable between two buffer distances (e.g., 150m vs 50m).\n* **Ratio features** — the **relative change**, or how much larger/smaller a feature is at one radius compared to another.\n\nThese capture how urban form **transitions outward** from a location—an insight crucial to modeling Urban Heat Island intensity.\n\n\n### **Why It Matters for UHI**\n\nThe UHI effect isn’t just driven by how dense or built-up a place is, but also by **how quickly** those characteristics change with distance.\n\n* A location surrounded by green space at 50m but dense development at 150m will trap and radiate heat differently than one that's uniformly urban.\n* **Delta features** quantify these **steepness gradients**, like how rapidly compactness or building density increases.\n* **Ratio features** highlight **scaling behavior**, e.g., how much more compact or built-up the area becomes as you expand the buffer.\n\nTogether, these engineered features allow the model to \"see\" the **spatial structure** of the urban form more clearly—something raw buffer values alone can’t fully capture.\n\n\n### Why This Strategy Is Powerful\n\n* **Captures transitions:** UHI depends not just on what’s nearby, but how the environment **changes** outward. This strategy quantifies those transitions.\n* **Adds context:** Raw values (e.g., `150m_mean_area`) lack context. But `delta_mean_area_150m_50m` tells us **how much** more built-up the area becomes—often more meaningful.\n* **Boosts predictive insight:** These engineered features offer the model **new dimensions of urban form**, which may correlate better with localized heat retention or dispersion patterns.","metadata":{}},{"cell_type":"code","source":"def generate_buffer_engineered_features(df_filtered):\n    \"\"\"\n    Generates delta and ratio features from buffer-based features in a filtered DataFrame.\n\n    Parameters:\n        df_filtered (pd.DataFrame): Filtered DataFrame containing buffer features and possibly other columns.\n\n    Returns:\n        pd.DataFrame: DataFrame with original columns + newly created delta/ratio features.\n    \"\"\"\n\n    # --- Helper function to parse buffer feature names ---\n    def parse_buffer_feature_name(col_name):\n        if 'm_' in col_name:\n            parts = col_name.split('m_')\n            try:\n                distance = int(parts[0])\n                feature_type = parts[1]\n                return distance, feature_type\n            except ValueError:\n                return None, None\n        return None, None\n\n    # --- Step 1: Identify buffer columns and map feature types to buffer distances ---\n    current_buffer_cols = [col for col in df_filtered.columns if 'm_' in col]\n\n    feature_type_to_distances = {}\n    for col in current_buffer_cols:\n        distance, feature_type = parse_buffer_feature_name(col)\n        if distance is not None and feature_type is not None:\n            feature_type_to_distances.setdefault(feature_type, []).append(distance)\n\n    for ft in feature_type_to_distances:\n        feature_type_to_distances[ft].sort()\n\n    print(\"Identified feature types and their available buffer distances:\")\n    for ft, dists in feature_type_to_distances.items():\n        print(f\"  {ft}: {dists}m\")\n\n    # --- Step 2: Create delta and ratio features ---\n    new_engineered_features_df = pd.DataFrame(index=df_filtered.index)\n    epsilon = 1e-6\n\n    print(\"\\nGenerating new delta and ratio features...\")\n    for feature_type, distances in feature_type_to_distances.items():\n        if len(distances) < 2:\n            continue\n\n        for i in range(len(distances)):\n            for j in range(i + 1, len(distances)):\n                dist1 = distances[i]\n                dist2 = distances[j]\n\n                col1_name = f\"{dist1}m_{feature_type}\"\n                col2_name = f\"{dist2}m_{feature_type}\"\n\n                if col1_name in df_filtered.columns and col2_name in df_filtered.columns:\n                    delta_col_name = f\"delta_{feature_type}_{dist2}m_{dist1}m\"\n                    ratio_col_name = f\"ratio_{feature_type}_{dist2}m_{dist1}m\"\n\n                    new_engineered_features_df[delta_col_name] = df_filtered[col2_name] - df_filtered[col1_name]\n                    new_engineered_features_df[ratio_col_name] = df_filtered[col2_name] / (df_filtered[col1_name] + epsilon)\n\n    print(f\"\\nSuccessfully created {new_engineered_features_df.shape[1]} new engineered features.\")\n\n    # --- Step 3: Combine with original filtered DataFrame ---\n    df_with_engineered = pd.concat([df_filtered, new_engineered_features_df], axis=1)\n\n    print(f\"New DataFrame shape: {df_with_engineered.shape}\")\n    return df_with_engineered\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:20:54.233176Z","iopub.execute_input":"2025-08-01T14:20:54.233562Z","iopub.status.idle":"2025-08-01T14:20:54.244166Z","shell.execute_reply.started":"2025-08-01T14:20:54.233529Z","shell.execute_reply":"2025-08-01T14:20:54.242713Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the function for train\nenhanced_train = generate_buffer_engineered_features(train_enhanced_fil)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:20:56.940260Z","iopub.execute_input":"2025-08-01T14:20:56.940594Z","iopub.status.idle":"2025-08-01T14:20:56.969193Z","shell.execute_reply.started":"2025-08-01T14:20:56.940569Z","shell.execute_reply":"2025-08-01T14:20:56.968130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use the same function for test\nenhanced_test = generate_buffer_engineered_features(test_enhanced_fil)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:22:13.233312Z","iopub.execute_input":"2025-08-01T14:22:13.233678Z","iopub.status.idle":"2025-08-01T14:22:13.260958Z","shell.execute_reply.started":"2025-08-01T14:22:13.233652Z","shell.execute_reply":"2025-08-01T14:22:13.259863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def move_target_to_end(df, target_col='UHI Index'):\n    \"\"\"\n    Moves the target column to the end of the DataFrame.\n    \"\"\"\n    if target_col in df.columns:\n        cols = [col for col in df.columns if col != target_col] + [target_col]\n        return df[cols]\n    else:\n        print(f\"Warning: '{target_col}' not found in DataFrame.\")\n        return df\n\nenhanced_train = move_target_to_end(enhanced_train)\nenhanced_test = move_target_to_end(enhanced_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:22:41.245894Z","iopub.execute_input":"2025-08-01T14:22:41.246230Z","iopub.status.idle":"2025-08-01T14:22:41.255942Z","shell.execute_reply.started":"2025-08-01T14:22:41.246205Z","shell.execute_reply":"2025-08-01T14:22:41.255044Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Final Feature Selection (Using VIF + Domain Knowledge on Full Feature Set)\n\nThis refined feature set was produced using the same iterative VIF-based approach from **Step 4**, but applied to the **entire pool of available features**—including original meteorological variables, spatial buffer features, and newly engineered delta/ratio features. The goal was to retain the most informative and least redundant features for predicting the Urban Heat Island (UHI) effect.\n\n\n### Overview of the Final Selected Features\n#### **Meteorological & Temporal Features**\n* `Temp_Change_30min`\n* `Wind Direction degrees`\n* `Avg Wind Speed m/s`\n* `Temp_1hr_MA`\n* `Solar Flux W/m^2`\n* `Air Temp at Surface degC`\n\nThese are essential for capturing **instantaneous and short-term environmental conditions** that influence UHI expression. They provide direct measurements of atmosphere, radiation, and temperature change that interact with the built environment.\n\n\n#### **Raw Buffer Features**\n* `50m_count_area`\n* `50m_std_compactness`\n* `50m_median_avg_neighbor_dist_ft`\n* `100m_median_area`\n* `100m_sum_area`\n* `150m_std_compactness`\n* `150m_std_avg_neighbor_dist_ft`\n* `200m_std_area`\n* `200m_sum_area`\n\nThese features represent **localized urban morphology** at different buffer scales. Key takeaways:\n\n* **Smaller buffer sizes (50m, 100m)** still contribute fine-grained detail about immediate surroundings.\n* **Larger buffers (150m–200m)** capture broader urban context, especially variability in building form (`std_*`) and spatial layout (`avg_neighbor_dist_ft`).\n* The prevalence of **standard deviation metrics** again underscores the importance of **urban heterogeneity**.\n  \n\n#### **Engineered Gradient Features (Delta & Ratio)**\nThese capture **spatial transitions** in urban structure across different distances—core to understanding how UHI builds or dissipates:\n\n* **Building Count Transitions:**\n  * `delta_count_area_200m_50m`\n  * `ratio_count_area_200m_50m`\n \n* **Building Size Transitions:**\n  * `ratio_median_area_100m_50m`\n  * `ratio_median_area_200m_50m`\n  * `delta_median_area_200m_50m`\n  * `ratio_median_area_200m_100m`\n\n* **Compactness Transitions:**\n  * `ratio_std_compactness_100m_50m`\n  * `ratio_std_compactness_150m_100m`\n\n* **Spacing Variability Transitions (Avg Neighbor Distance):**\n  * `ratio_median_avg_neighbor_dist_ft_150m_50m`\n  * `delta_std_avg_neighbor_dist_ft_100m_50m`\n  * `ratio_std_avg_neighbor_dist_ft_100m_50m`\n  * `ratio_std_avg_neighbor_dist_ft_150m_100m`\n  * `delta_std_avg_neighbor_dist_ft_200m_100m`\n  * `ratio_std_avg_neighbor_dist_ft_200m_150m`\n  * `ratio_mean_avg_neighbor_dist_ft_200m_100m`\n\nThese features provide critical context for **how urban form shifts with distance**—an essential characteristic of the UHI phenomenon. Steep increases in building count or compactness may correlate with intensified heat buildup, while smoother transitions may reflect better airflow or heat dissipation.\n\n\n### Key Insights\n\n* **Multiscale Urban Context is Retained:** The final set includes features across **all buffer distances** (50m to 200m), suggesting that different urban processes act at different spatial scales.\n\n* **Heterogeneity > Averages:** Standard deviation and transition metrics were favored over simple mean or sum values, highlighting the value of **urban variability and gradients** over static descriptors.\n\n* **No Redundancy, Maximum Diversity:** The VIF-filtering process ensured that only **unique, non-collinear signals** remain—whether from meteorological inputs or urban form metrics.\n\n* **Engineered Features Add Value:** The survival of multiple delta/ratio features confirms that capturing **how urban features change with distance** is not only conceptually important but also statistically unique and predictive.\n\n\n### Conclusion\n\nThis final feature set provides a robust, diverse, and interpretable foundation for UHI modeling. It combines **environmental measurements**, **multi-scale urban descriptors**, and **spatial transition features** in a way that reflects both domain expertise and statistical rigor. The result is a dataset that captures the complexity of UHI dynamics without redundancy—ready for reliable and insightful modeling.","metadata":{}},{"cell_type":"code","source":"# Rerun the function with your enhanced_train DataFrame\nfinal_features, vif_report = vif_feature_selection(\n    df=enhanced_train,\n    vif_threshold=5.0\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:33:25.802735Z","iopub.execute_input":"2025-08-01T14:33:25.803162Z","iopub.status.idle":"2025-08-01T14:33:46.777151Z","shell.execute_reply.started":"2025-08-01T14:33:25.803129Z","shell.execute_reply":"2025-08-01T14:33:46.776303Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"\\nNumber of features after second VIF\")\nlen(final_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:34:00.247343Z","iopub.execute_input":"2025-08-01T14:34:00.247847Z","iopub.status.idle":"2025-08-01T14:34:00.255991Z","shell.execute_reply.started":"2025-08-01T14:34:00.247801Z","shell.execute_reply":"2025-08-01T14:34:00.254345Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_features","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T14:43:22.222300Z","iopub.execute_input":"2025-08-01T14:43:22.222697Z","iopub.status.idle":"2025-08-01T14:43:22.229242Z","shell.execute_reply.started":"2025-08-01T14:43:22.222668Z","shell.execute_reply":"2025-08-01T14:43:22.228187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cross-Validation Results & Interpretation\n\nThis step evaluates the final model performance after applying a full-feature selection pipeline that includes meteorological, temporal, spatial buffer features, and engineered gradient features (delta/ratio). The cross-validation process provides a robust measure of **how well these features predict the Urban Heat Island (UHI) Index** across unseen data.\n\n\n### Why Cross-Validation Was Used\n\nCross-validation ensures model reliability by:\n* Averaging performance over multiple data splits.\n* Preventing overfitting on a single train-test split.\n* Validating whether feature engineering and selection strategies generalize well.\n\nThis is especially important for the UHI prediction task, where relationships between variables can be highly **non-linear** and **location-dependent**.\n\n\n### Model Performance (R² Scores)\n\n| Model                    | Mean R² | Std Dev | Interpretation                                                                                               |\n| ------------------------ | ------- | ------- | ------------------------------------------------------------------------------------------------------------ |\n| **XGBoost**              | 0.8709  | 0.0124  |  Best performer; consistently accurate across folds. Captures complex non-linear patterns in the UHI data. |\n| **LightGBM**             | 0.8642  | 0.0094  | Very close to XGBoost, with excellent consistency and performance.                                           |\n| **HistGradientBoosting** | 0.8620  | 0.0098     | Solid performance; confirms gradient boosting is ideal for this problem.                                     |\n| **Random Forest**        | 0.7448  | 0.0143     | Good, but underperforms compared to boosting models. Less effective at capturing UHI complexity.             |\n| **ElasticNet (Linear)**  | 0.1189  | 0.0162     | Poor fit. Confirms that linear relationships alone cannot explain UHI patterns well.                         |\n\n> **Key Insight:** The UHI prediction task is **highly non-linear**, as shown by the stark contrast between tree-based models and the linear ElasticNet. Gradient boosting models capture the complex interactions between urban form and meteorology far better.\n\n\n### Feature Importance: What Drives UHI?\n\nUsing an aggregated feature importance analysis across models, the following emerged as the **most influential predictors**:\n\n| Rank | Feature                          | Mean Importance | Interpretation                                                                                                                                   |\n| ---- | -------------------------------- | --------------- | ------------------------------------------------------------------------------------------------------------------------------------------------ |\n| 1  | `200m_mean_avg_neighbor_dist_ft` | **0.80**        | Most powerful predictor. Captures how tightly or loosely buildings are spaced at a city-block scale. Affects air circulation and heat retention. |\n| 2  | `200m_median_area`               | **0.53**        | Building size at the 200m scale. Larger or irregularly spaced buildings affect solar exposure and heat absorption.                               |\n| 3  | `Temp_1hr_MA`                    | **0.47**        | Recent ambient temperature. Key meteorological driver for UHI readings.                                                                          |\n| 4  | `200m_std_area`                  | **0.40**        | Building size variability. High variability indicates a heterogeneous urban texture, which influences airflow and thermal patterns.              |\n| 5  | `Air Temp at Surface degC`       | **0.40**        | Direct surface air temperature. Reinforces the meteorological component of UHI.                                                                  |\n\n### What This Tells Us\n\n* **Urban Morphology at the 200m Scale Dominates:** Most of the top features are from the **200m buffer**, underscoring that UHI is not just a hyperlocal phenomenon. Broader spatial context (like how buildings are spaced and sized at the neighborhood level) is critical.\n\n* **Engineered Features Add Value:** Though not in the top 5, engineered **delta and ratio features** helped models reach high accuracy by encoding spatial gradients—offering a way to quantify urban transitions.\n\n* **Meteorology Still Matters:** Environmental conditions like temperature and solar flux remain foundational UHI inputs, but they’re **greatly enhanced** when contextualized with spatial features.\n\n\n### Final Conclusion\n\nThis cross-validation analysis confirms that the model is both **powerful** and **generalizable**, achieving **R² scores above 0.86** using features that are:\n\n* Scientifically grounded (urban morphology, meteorology),\n* Statistically vetted (via VIF and non-redundancy filtering),\n* Rich in spatial context (via buffer features),\n* Sophisticated (via engineered gradient features).","metadata":{}},{"cell_type":"code","source":"# Final selected features\nfinal_features = ['Avg Wind Speed m/s', 'Temp_Change_30min', 'Temp_1hr_MA', 'Wind Direction degrees',\n                  'Solar Flux W/m^2', 'Air Temp at Surface degC', '50m_count_area', '50m_std_compactness',\n                  '50m_median_avg_neighbor_dist_ft', '100m_median_area', '100m_sum_area', '150m_std_compactness',\n                  '150m_std_avg_neighbor_dist_ft', '200m_std_area', '200m_sum_area', 'delta_count_area_200m_50m',\n                  'ratio_count_area_200m_50m', 'ratio_median_area_100m_50m', 'delta_median_area_200m_50m', 'ratio_median_area_200m_50m',\n                  'ratio_median_area_200m_100m', 'ratio_std_compactness_100m_50m', 'ratio_std_compactness_150m_100m', 'ratio_median_avg_neighbor_dist_ft_150m_50m',\n                  'delta_std_avg_neighbor_dist_ft_100m_50m', 'ratio_std_avg_neighbor_dist_ft_100m_50m', 'ratio_std_avg_neighbor_dist_ft_150m_100m', 'delta_std_avg_neighbor_dist_ft_200m_100m',\n                  'ratio_std_avg_neighbor_dist_ft_200m_150m', 'ratio_mean_avg_neighbor_dist_ft_200m_100m'\n                 ]\n\n# Define features and target\nX = enhanced_train[final_features]\ny = enhanced_train['UHI Index']\n\n# Define models\nmodels = {\n    'RandomForest': RandomForestRegressor(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1),\n    'HistGB': HistGradientBoostingRegressor(max_iter=200, learning_rate=0.1, random_state=42),\n    'XGBoost': xgb.XGBRegressor(n_estimators=200, learning_rate=0.1, max_depth=6, random_state=42, n_jobs=-1),\n    'LightGBM': lgb.LGBMRegressor(n_estimators=200, learning_rate=0.1, max_depth=10, random_state=42, n_jobs=-1),\n    'ElasticNet': ElasticNet(alpha=0.01, l1_ratio=0.5, random_state=42)\n}\n\n# Cross-validation\ncv = KFold(n_splits=5, shuffle=True, random_state=42)\nr2_scores = {}\n\nprint(\"--- Training Models with 5-Fold Cross-Validation ---\")\nfor name, model in models.items():\n    print(f\"\\n{name} in progress...\")\n    scores = cross_val_score(model, X, y, cv=cv, scoring='r2', n_jobs=-1)\n    r2_scores[name] = np.mean(scores)\n    print(f\"  Mean R2: {r2_scores[name]:.4f} | Std: {np.std(scores):.4f}\")\n\n# Function to compute and normalize importance\ndef compute_multi_model_feature_importance(X, y, models_dict, top_n=10):\n    importance_dfs = []\n\n    for name, model in models_dict.items():\n        print(f\"\\nFitting {name} for feature importance...\")\n        model.fit(X, y)\n        \n        if hasattr(model, \"feature_importances_\"):\n            importance = model.feature_importances_\n        elif hasattr(model, \"coef_\"):\n            importance = np.abs(model.coef_).flatten()\n        else:\n            print(f\"Using permutation importance for {name}...\")\n            result = permutation_importance(model, X, y, n_repeats=10, random_state=42, n_jobs=-1)\n            importance = result.importances_mean\n\n        # Normalize importance\n        normalized = importance / np.max(importance)\n        df = pd.DataFrame({\n            'Feature': X.columns,\n            f'{name}_Importance': normalized\n        })\n        importance_dfs.append(df)\n\n    from functools import reduce\n    merged_df = reduce(lambda left, right: pd.merge(left, right, on='Feature'), importance_dfs)\n\n    # Mean importance\n    importance_cols = [col for col in merged_df.columns if col.endswith('_Importance')]\n    merged_df['Mean_Importance'] = merged_df[importance_cols].mean(axis=1)\n\n    return merged_df.sort_values(by='Mean_Importance', ascending=False)\n\n# --- Feature Importance Across Models ---\nprint(\"\\n--- Aggregating Feature Importance Across All Models ---\")\nimportance_df = compute_multi_model_feature_importance(X, y, models)\n\n# Show top features\nprint(\"\\nTop 5 Most Consistently Important Features:\")\nprint(importance_df[['Feature', 'Mean_Importance']].head(5).to_string(index=False))\n\n# --- R2 Score Table ---\nprint(\"\\nFinal Cross-Validation R2 Scores:\")\nr2_scores_df = pd.DataFrame(r2_scores.items(), columns=['Model', 'R2']).set_index('Model').sort_values(by='R2', ascending=False)\nprint(r2_scores_df.to_string(float_format=\"%.4f\"))\n\n# --- Feature Importance Plot ---\nplt.figure(figsize=(12, 8))\nsns.barplot(x='Mean_Importance', y='Feature', data=importance_df.head(20))\nplt.title('Top 20 Consistently Important Features Across Models')\nplt.xlabel('Normalized Importance')\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Importance Analysis & Selection Strategy (Pre-Satellite Integration)\n\nAfter running a full-feature model using all 28 selected predictors, feature importance scores were computed to evaluate each feature's contribution to predicting the **Urban Heat Island (UHI) Index**. These importance scores are essential for refining the feature set before integrating satellite-derived variables.\n\n\n### Why Feature Pruning is Important\n\nBefore introducing new satellite-based features, it’s critical to:\n\n* **Retain the strongest, most informative features** to anchor the model.\n* **Remove weak or redundant features** to reduce noise and prevent overfitting.\n* **Ensure interpretability and efficiency** as the feature space grows.\n\nThis makes room for new satellite data to contribute meaningfully without being drowned out by weak, correlated, or unnecessary predictors.\n\n\n###  Summary of Feature Importance Results\n\n| Rank  | Feature                                      | Mean Importance                           |\n| ----- | -------------------------------------------- | ----------------------------------------- |\n| 1     | `200m_mean_avg_neighbor_dist_ft`             | **0.80**                                  |\n| 2     | `200m_median_area`                           | **0.53**                                  |\n| 3     | `Temp_1hr_MA`                                | **0.47**                                  |\n| 4     | `200m_std_area`                              | **0.40**                                  |\n| 5     | `Air Temp at Surface degC`                   | **0.40**                                  |\n| 6     | `delta_median_avg_neighbor_dist_ft_150m_50m` | **0.33**                                  |\n| 7     | `Wind Direction degrees`                     | **0.30**                                  |\n| 8     | `delta_count_area_200m_50m`                  | **0.26**                                  |\n| 9     | `delta_std_avg_neighbor_dist_ft_150m_100m`   | **0.26**                                  |\n| 10    | `150m_std_avg_neighbor_dist_ft`              | **0.26**                                  |\n| 11    | `Temp_Change_30min`                          | **0.25**                                  |\n| 12    | `150m_std_compactness`                       | **0.24**                                  |\n| 13    | `50m_median_area`                            | **0.21**                                  |\n| 14    | `100m_sum_area`                              | **0.20**                                  |\n| 15–28 | Remaining 14 features                        | **< 0.18** (all below the 0.20 threshold) |\n\n\n### Selected Features\n\n**Selected based on an importance threshold of ≥ 0.20**, these 14 features represent the most valuable predictors in the current UHI model:\n\n#### Spatial Features:\n* `200m_mean_avg_neighbor_dist_ft`\n* `200m_median_area`\n* `200m_std_area`\n* `150m_std_avg_neighbor_dist_ft`\n* `150m_std_compactness`\n* `100m_sum_area`\n* `50m_median_area`\n\n#### Meteorological Features:\n* `Temp_1hr_MA`\n* `Air Temp at Surface degC`\n* `Temp_Change_30min`\n* `Wind Direction degrees`\n\n#### Engineered Gradient Features:\n* `delta_median_avg_neighbor_dist_ft_150m_50m`\n* `delta_count_area_200m_50m`\n* `delta_std_avg_neighbor_dist_ft_150m_100m`\n\n**Why these were chosen:**\n* **Spatial Scale Matters:** Features from the **200m buffer** dominate in importance, confirming that **broader urban morphology** has a stronger relationship with UHI than smaller-scale metrics.\n* **Spatial Arrangement is Key:** Metrics like `mean_avg_neighbor_dist_ft`, `std_area`, and `std_avg_neighbor_dist_ft` capture the **heterogeneity and density** of the urban fabric, which heavily influences heat retention and airflow.\n* **Thermal History & Meteorology Count:** `Temp_1hr_MA` and `Air Temp at Surface` are expectedly strong predictors because they directly describe the thermal state of the environment.\n* **Gradient Features are Effective:** The strong performance of **delta features** (e.g., `delta_count_area_200m_50m`) confirms that **rates of change across distances** enhance the model’s understanding of spatial transitions — a known driver of UHI effects.\n\n### **Why the other features were removed:**\n\n* **Low Impact:** These features contributed minimally to the model’s performance.\n* **Redundant or Less Informative:** Their information is likely captured better by related features that remain in the model. For example:\n\n  * `delta_count_area_200m_50m` (0.26) offers a clearer spatial transition than `ratio_count_area_200m_50m` (0.09).\n  * `50m_count_area` is outperformed by `50m_median_area` (0.21), which provides a richer description of urban form.\n\n\n###  Moving Forward\n\nWith a refined, high-signal feature set in place, the next phase will integrate **satellite-derived variables**. This step is now more strategic because:\n\n1. **Noise has been eliminated**, improving the model’s ability to learn from new variables.\n2. **The current features are interpretable**, which will allow clearer attribution of improvements once satellite data is added.\n3. **Key diversity preserved key diversity**: Spatial form, thermal conditions, and engineered transitions are all still well represented.\n\nThis lean yet powerful feature base sets a strong foundation for building a **multi-modal UHI model** that fuses ground-level data with remote sensing insights.","metadata":{}},{"cell_type":"code","source":"retained_features = [\n    # 🌇 Spatial Features\n    \"200m_mean_avg_neighbor_dist_ft\", \"200m_median_area\", \"200m_std_area\",\n    \"150m_std_avg_neighbor_dist_ft\", \"150m_std_compactness\", \"100m_sum_area\",\"50m_median_area\",\n    # 🌡️ Meteorological Featuresa\n    \"Temp_1hr_MA\", \"Air Temp at Surface degC\", \"Temp_Change_30min\", \"Wind Direction degrees\",\n    # 🔧 Engineered Gradient Features\n    \"delta_median_avg_neighbor_dist_ft_150m_50m\", \"delta_count_area_200m_50m\", \"delta_std_avg_neighbor_dist_ft_150m_100m\"\n]\nprint(\"\\nNumber of features after second VIF\")\nlen(retained_features)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-01T00:15:29.499622Z","iopub.execute_input":"2025-08-01T00:15:29.501448Z","iopub.status.idle":"2025-08-01T00:15:29.513747Z","shell.execute_reply.started":"2025-08-01T00:15:29.501342Z","shell.execute_reply":"2025-08-01T00:15:29.512485Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = [\"datetime\", \"Location\", \"200m_mean_avg_neighbor_dist_ft\", \"200m_median_area\", \n            \"200m_std_area\", \"150m_std_avg_neighbor_dist_ft\", \"150m_std_compactness\", \"100m_sum_area\",\n            \"50m_median_area\", \"Temp_1hr_MA\", \"Air Temp at Surface degC\", \"Temp_Change_30min\", \n            \"Wind Direction degrees\", \"delta_median_avg_neighbor_dist_ft_150m_50m\", \"delta_count_area_200m_50m\", \"delta_std_avg_neighbor_dist_ft_150m_100m\", \"UHI Index\"\n           ]\n\n\ndf_test = [\"datetime\", \"Location\", \"200m_mean_avg_neighbor_dist_ft\", \"200m_median_area\", \n            \"200m_std_area\", \"150m_std_avg_neighbor_dist_ft\", \"150m_std_compactness\", \"100m_sum_area\",\n            \"50m_median_area\", \"Temp_1hr_MA\", \"Air Temp at Surface degC\", \"Temp_Change_30min\", \n            \"Wind Direction degrees\", \"delta_median_avg_neighbor_dist_ft_150m_50m\", \"delta_count_area_200m_50m\", \"delta_std_avg_neighbor_dist_ft_150m_100m\", \"UHI Index\"\n           ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reset the index of train_coords to match the order of selected_features_df\ntrain_coords = train_coords.reset_index(drop=True)\n\n# Now check\nassert df_train.index.equals(train_coords.index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reset the index of test_coords to match the order of final_test1\ntest_coords = test_coords.reset_index(drop=True)\n\n# Now check\nassert df_test.index.equals(test_coords.index)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}